"""Google ADK ë„êµ¬ë¡œ ë˜í•‘ëœ ë¶„ì„ í•¨ìˆ˜ë“¤"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import json
from datetime import datetime
from typing import Dict, Any
import warnings
import os
warnings.filterwarnings('ignore')

# from google.adk.tools import FunctionTool

# ë¡œê±° ì„¤ì •
import logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
OUTPUT_DIR = "outputs"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ê³µí†µ í•¨ìˆ˜: ë‚ ì§œë³„ ë¦¬í¬íŠ¸ í´ë” ìƒì„±
def get_reports_dir():
    """outputs/reports/{ì˜¤ëŠ˜ë‚ ì§œ} í´ë” ê²½ë¡œë¥¼ ë°˜í™˜í•˜ê³  í´ë”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    today = datetime.now().strftime('%Y%m%d')
    reports_dir = f"{OUTPUT_DIR}/reports/{today}"
    os.makedirs(reports_dir, exist_ok=True)
    return reports_dir

# =============================================================================
# ê²½ì˜ì§„ìš© ë³´ê³ ì„œ ë°ì´í„° ì •ì œí™” Tools
# =============================================================================

def structure_llm_analysis_for_html(csv_file_path: str, llm_analysis_result: str) -> str:
    """LLM ë¶„ì„ ê²°ê³¼ë¥¼ HTML ê·œê²©ì— ë§ê²Œ êµ¬ì¡°í™” - ë¹„í™œì„±í™”ë¨
    
    Args:
        csv_file_path: CSV íŒŒì¼ ê²½ë¡œ
        llm_analysis_result: LLM Agentì˜ ë¶„ì„ ê²°ê³¼ (í…ìŠ¤íŠ¸)
    
    Returns:
        ë¹„í™œì„±í™”ë¨ - í•­ìƒ "ë¶„ì„ ì¤‘" ë°˜í™˜
    """
    # í•¨ìˆ˜ ë¹„í™œì„±í™” - ì›ë³¸ llm_analysisë§Œ ì‚¬ìš©
    return "ë¶„ì„ ì¤‘"

def prepare_category_analysis_data(csv_file_path: str) -> Dict[str, Any]:
    """ì¹´í…Œê³ ë¦¬ ë¶„ì„ìš© ë°ì´í„° ì •ì œí™” (Lift ê¸°ë°˜)"""
    try:
        df = pd.read_csv(csv_file_path)
        
        # Lift ê³„ì‚° (ì‹¤í—˜êµ° - ëŒ€ì¡°êµ°)
        df['lift'] = df['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'] - df['ëŒ€ì¡°êµ°_ì˜ˆì•½ì „í™˜ìœ¨']
        
        # ì •ì œëœ ë°ì´í„° êµ¬ì„±
        analysis_data = {
            "campaigns": df[['ëª©ì ', 'íƒ€ê²Ÿ', 'ë¬¸êµ¬', 'í¼ë„', 'ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨', 'ëŒ€ì¡°êµ°_ì˜ˆì•½ì „í™˜ìœ¨', 'lift', 'ì‹¤í—˜êµ°_ë°œì†¡', 'ì‹¤í—˜êµ°_1ì¼ì´ë‚´_ì˜ˆì•½ìƒì„±']].to_dict('records'),
            "summary_stats": {
                "total_campaigns": len(df),
                "unique_purposes": df['ëª©ì '].nunique(),
                "unique_targets": df['íƒ€ê²Ÿ'].nunique(),
                "unique_funnels": df['í¼ë„'].nunique(),
                "avg_experiment_conversion": round(df['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].mean(), 0),
                "avg_control_conversion": round(df['ëŒ€ì¡°êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].mean(), 0),
                "avg_lift": round(df['lift'].mean(), 1),
                "lift_range": [round(df['lift'].min(), 1), round(df['lift'].max(), 1)],
                "positive_lift_count": len(df[df['lift'] > 0]),
                "negative_lift_count": len(df[df['lift'] < 0])
            },
            "funnel_breakdown": df.groupby('í¼ë„').agg({
                'ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨': 'mean',
                'ëŒ€ì¡°êµ°_ì˜ˆì•½ì „í™˜ìœ¨': 'mean',
                'lift': 'mean',
                'ì‹¤í—˜êµ°_ë°œì†¡': 'sum',
                'ì‹¤í—˜êµ°_1ì¼ì´ë‚´_ì˜ˆì•½ìƒì„±': 'sum'
            }).reset_index().assign(campaign_count=lambda x: x.groupby('í¼ë„')['í¼ë„'].transform('count')).round(0).to_dict(),
            "purpose_breakdown": df.groupby('ëª©ì ').agg({
                'ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨': 'mean',
                'ëŒ€ì¡°êµ°_ì˜ˆì•½ì „í™˜ìœ¨': 'mean',
                'lift': 'mean',
                'ì‹¤í—˜êµ°_ë°œì†¡': 'sum',
                'ì‹¤í—˜êµ°_1ì¼ì´ë‚´_ì˜ˆì•½ìƒì„±': 'sum'
            }).reset_index().assign(campaign_count=lambda x: x.groupby('ëª©ì ')['ëª©ì '].transform('count')).round(0).to_dict()
        }
        
        return analysis_data
        
    except Exception as e:
        logger.error(f"ì¹´í…Œê³ ë¦¬ ë¶„ì„ ë°ì´í„° ì¤€ë¹„ ì˜¤ë¥˜: {str(e)}")
        return {}

def prepare_funnel_segment_data(csv_file_path: str) -> Dict[str, Any]:
    """í¼ë„ ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„ìš© ë°ì´í„° ì •ì œí™” (Lift ê¸°ë°˜)"""
    try:
        # CSV íŒŒì‹± ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•œ ì˜µì…˜ ì¶”ê°€
        df = pd.read_csv(csv_file_path, encoding='utf-8', on_bad_lines='skip')
        
        # Lift ê³„ì‚°
        df['lift'] = df['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'] - df['ëŒ€ì¡°êµ°_ì˜ˆì•½ì „í™˜ìœ¨']
        
        # í¼ë„ë³„ Lift ë¶„ìœ„ìˆ˜ ê³„ì‚°
        funnel_segments = {}
        for funnel in df['í¼ë„'].unique():
            if pd.isna(funnel):
                continue
                
            funnel_data = df[df['í¼ë„'] == funnel]
            
            # Lift ê¸°ì¤€ 3ë¶„ìœ„ìˆ˜ ê³„ì‚°
            q33 = funnel_data['lift'].quantile(0.33)
            q67 = funnel_data['lift'].quantile(0.67)
            
            # ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¥˜ (Lift ê¸°ì¤€)
            high_performers = funnel_data[funnel_data['lift'] >= q67]
            mid_performers = funnel_data[(funnel_data['lift'] >= q33) & (funnel_data['lift'] < q67)]
            low_performers = funnel_data[funnel_data['lift'] < q33]
            
            funnel_segments[funnel] = {
                "high_performers": {
                    "data": high_performers[['ë¬¸êµ¬', 'ëª©ì ', 'íƒ€ê²Ÿ', 'ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨', 'ëŒ€ì¡°êµ°_ì˜ˆì•½ì „í™˜ìœ¨', 'lift']].to_dict('records'),
                    "count": len(high_performers),
                    "avg_experiment_conversion": round(high_performers['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].mean(), 0),
                    "avg_control_conversion": round(high_performers['ëŒ€ì¡°êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].mean(), 0),
                    "avg_lift": round(high_performers['lift'].mean(), 1),
                    "lift_range": [round(high_performers['lift'].min(), 1), round(high_performers['lift'].max(), 1)]
                },
                "mid_performers": {
                    "data": mid_performers[['ë¬¸êµ¬', 'ëª©ì ', 'íƒ€ê²Ÿ', 'ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨', 'ëŒ€ì¡°êµ°_ì˜ˆì•½ì „í™˜ìœ¨', 'lift']].to_dict('records'),
                    "count": len(mid_performers),
                    "avg_experiment_conversion": round(mid_performers['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].mean(), 0),
                    "avg_control_conversion": round(mid_performers['ëŒ€ì¡°êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].mean(), 0),
                    "avg_lift": round(mid_performers['lift'].mean(), 1),
                    "lift_range": [round(mid_performers['lift'].min(), 1), round(mid_performers['lift'].max(), 1)]
                },
                "low_performers": {
                    "data": low_performers[['ë¬¸êµ¬', 'ëª©ì ', 'íƒ€ê²Ÿ', 'ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨', 'ëŒ€ì¡°êµ°_ì˜ˆì•½ì „í™˜ìœ¨', 'lift']].to_dict('records'),
                    "count": len(low_performers),
                    "avg_experiment_conversion": round(low_performers['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].mean(), 0),
                    "avg_control_conversion": round(low_performers['ëŒ€ì¡°êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].mean(), 0),
                    "avg_lift": round(low_performers['lift'].mean(), 1),
                    "lift_range": [round(low_performers['lift'].min(), 1), round(low_performers['lift'].max(), 1)]
                }
            }
        
        return {
            "funnel_segments": funnel_segments,
            "total_funnels": len(df['í¼ë„'].unique()),
            "overall_stats": {
                "avg_experiment_conversion": round(df['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].mean(), 0),
                "avg_control_conversion": round(df['ëŒ€ì¡°êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].mean(), 0),
                "avg_lift": round(df['lift'].mean(), 1),
                "lift_std": round(df['lift'].std(), 1)
            }
        }
        
    except Exception as e:
        logger.error(f"í¼ë„ ì„¸ê·¸ë¨¼íŠ¸ ë°ì´í„° ì¤€ë¹„ ì˜¤ë¥˜: {str(e)}")
        return {}

def analyze_category_performance_tool(csv_file_path: str) -> str:
    """ì¹´í…Œê³ ë¦¬ë³„ ì„±ê³¼ ë¶„ì„ (Lift ê¸°ë°˜) - ë°ì´í„° ì •ì œí™”ë§Œ ìˆ˜í–‰"""
    try:
        # ë°ì´í„° ì •ì œí™”
        analysis_data = prepare_category_analysis_data(csv_file_path)
        
        if not analysis_data:
            return "ì¹´í…Œê³ ë¦¬ ë¶„ì„ ë°ì´í„° ì¤€ë¹„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
        
        # ì •ì œëœ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì €ì¥
        reports_dir = get_reports_dir()
        datetime_prefix = get_datetime_prefix()
        filename = f"{reports_dir}/{datetime_prefix}_category_analysis_data.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(analysis_data, f, ensure_ascii=False, indent=2)
        
        return f"ì¹´í…Œê³ ë¦¬ ë¶„ì„ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {filename}\n\nì •ì œëœ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. Agentê°€ ì´ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
        
    except Exception as e:
        return f"ì¹´í…Œê³ ë¦¬ ì„±ê³¼ ë¶„ì„ ì˜¤ë¥˜: {str(e)}"

def analyze_funnel_segment_strategy_tool(csv_file_path: str) -> str:
    """í¼ë„ë³„ ì„¸ê·¸ë¨¼íŠ¸ ì „ëµ ë¶„ì„ (Lift ê¸°ë°˜) - ë°ì´í„° ì •ì œí™”ë§Œ ìˆ˜í–‰"""
    try:
        # ë°ì´í„° ì •ì œí™”
        segment_data = prepare_funnel_segment_data(csv_file_path)
        
        if not segment_data:
            return "í¼ë„ ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„ ë°ì´í„° ì¤€ë¹„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
        
        # ì •ì œëœ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì €ì¥
        reports_dir = get_reports_dir()
        datetime_prefix = get_datetime_prefix()
        filename = f"{reports_dir}/{datetime_prefix}_funnel_segment_analysis_data.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(segment_data, f, ensure_ascii=False, indent=2)
        
        return f"í¼ë„ ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {filename}\n\nì •ì œëœ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. Agentê°€ ì´ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
        
    except Exception as e:
        return f"í¼ë„ ì„¸ê·¸ë¨¼íŠ¸ ì „ëµ ë¶„ì„ ì˜¤ë¥˜: {str(e)}"

def create_segment_lift_charts(csv_file_path: str) -> str:
    """ì„¸ê·¸ë¨¼íŠ¸ë³„ Lift ì°¨íŠ¸ ìƒì„± (í¼ë„ë³„, ì¹´í…Œê³ ë¦¬ë³„)"""
    try:
        print("ğŸ“Š ì„¸ê·¸ë¨¼íŠ¸ë³„ Lift ì°¨íŠ¸ ìƒì„± ì¤‘...")
        
        import matplotlib.pyplot as plt
        import seaborn as sns
        plt.rcParams['font.family'] = 'DejaVu Sans'
        
        df = pd.read_csv(csv_file_path)
        df['lift'] = df['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'] - df['ëŒ€ì¡°êµ°_ì˜ˆì•½ì „í™˜ìœ¨']
        
        reports_dir = get_reports_dir()
        datetime_prefix = get_datetime_prefix()
        chart_files = []
        
        # 1. í¼ë„ë³„ Lift ë¹„êµ ì°¨íŠ¸
        plt.figure(figsize=(15, 10))
        
        # 1-1. í¼ë„ë³„ í‰ê·  Lift ë§‰ëŒ€ì°¨íŠ¸
        plt.subplot(2, 3, 1)
        funnel_lift = df.groupby('í¼ë„')['lift'].mean().sort_values(ascending=False)
        colors = ['green' if x > 0 else 'red' for x in funnel_lift.values]
        bars = plt.bar(range(len(funnel_lift)), funnel_lift.values, color=colors, alpha=0.7)
        
        for i, (bar, value) in enumerate(zip(bars, funnel_lift.values)):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                    f'{value:.1f}%p', ha='center', va='bottom', fontsize=9)
        
        plt.title('í¼ë„ë³„ í‰ê·  Lift', fontsize=12, fontweight='bold')
        plt.xlabel('í¼ë„')
        plt.ylabel('Lift (%p)')
        plt.xticks(range(len(funnel_lift)), 
                  [f"{funnel[:10]}..." if len(funnel) > 10 else funnel 
                   for funnel in funnel_lift.index], rotation=45)
        plt.grid(axis='y', alpha=0.3)
        
        # 1-2. í¼ë„ë³„ ì‹¤í—˜êµ° vs ëŒ€ì¡°êµ° ì „í™˜ìœ¨ ë¹„êµ
        plt.subplot(2, 3, 2)
        comparison_data = df.groupby('í¼ë„')[['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨', 'ëŒ€ì¡°êµ°_ì˜ˆì•½ì „í™˜ìœ¨']].mean()
        comparison_data.plot(kind='bar', ax=plt.gca(), color=['skyblue', 'lightcoral'])
        plt.title('í¼ë„ë³„ ì‹¤í—˜êµ° vs ëŒ€ì¡°êµ° ì „í™˜ìœ¨')
        plt.xlabel('í¼ë„')
        plt.ylabel('ì „í™˜ìœ¨ (%)')
        plt.xticks(rotation=45)
        plt.legend()
        plt.grid(axis='y', alpha=0.3)
        
        # 1-3. Lift ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
        plt.subplot(2, 3, 3)
        plt.hist(df['lift'], bins=20, alpha=0.7, color='lightgreen', edgecolor='black')
        plt.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Baseline (0%p)')
        plt.title('Lift ë¶„í¬ íˆìŠ¤í† ê·¸ë¨')
        plt.xlabel('Lift (%p)')
        plt.ylabel('ë¹ˆë„')
        plt.legend()
        plt.grid(axis='y', alpha=0.3)
        
        # 1-4. í¼ë„ë³„ Lift ìƒìœ„/í•˜ìœ„ ì„¸ê·¸ë¨¼íŠ¸
        plt.subplot(2, 3, 4)
        funnel_segments = {}
        for funnel in df['í¼ë„'].unique():
            if pd.isna(funnel):
                continue
            funnel_data = df[df['í¼ë„'] == funnel]
            q33 = funnel_data['lift'].quantile(0.33)
            q67 = funnel_data['lift'].quantile(0.67)
            
            high_count = len(funnel_data[funnel_data['lift'] >= q67])
            mid_count = len(funnel_data[(funnel_data['lift'] >= q33) & (funnel_data['lift'] < q67)])
            low_count = len(funnel_data[funnel_data['lift'] < q33])
            
            funnel_segments[funnel] = {'high': high_count, 'mid': mid_count, 'low': low_count}
        
        # ìƒìœ„ 5ê°œ í¼ë„ë§Œ í‘œì‹œ
        top_funnels = sorted(funnel_segments.items(), key=lambda x: x[1]['high'], reverse=True)[:5]
        funnel_names = [f[0][:8] + '...' if len(f[0]) > 8 else f[0] for f in top_funnels]
        high_counts = [f[1]['high'] for f in top_funnels]
        mid_counts = [f[1]['mid'] for f in top_funnels]
        low_counts = [f[1]['low'] for f in top_funnels]
        
        x = range(len(funnel_names))
        plt.bar(x, high_counts, color='green', alpha=0.7, label='ìƒìœ„ (High)')
        plt.bar(x, mid_counts, bottom=high_counts, color='yellow', alpha=0.7, label='ì¤‘ìœ„ (Mid)')
        plt.bar(x, low_counts, bottom=[h+m for h, m in zip(high_counts, mid_counts)], 
                color='red', alpha=0.7, label='í•˜ìœ„ (Low)')
        
        plt.title('í¼ë„ë³„ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í¬ (ìƒìœ„ 5ê°œ)')
        plt.xlabel('í¼ë„')
        plt.ylabel('ìº í˜ì¸ ìˆ˜')
        plt.xticks(x, funnel_names, rotation=45)
        plt.legend()
        plt.grid(axis='y', alpha=0.3)
        
        # 1-5. ëª©ì ë³„ Lift ë¶„ì„
        plt.subplot(2, 3, 5)
        if 'ëª©ì ' in df.columns:
            purpose_lift = df.groupby('ëª©ì ')['lift'].mean().sort_values(ascending=False)
            colors = ['green' if x > 0 else 'red' for x in purpose_lift.values]
            bars = plt.bar(range(len(purpose_lift)), purpose_lift.values, color=colors, alpha=0.7)
            
            for i, (bar, value) in enumerate(zip(bars, purpose_lift.values)):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                        f'{value:.1f}%p', ha='center', va='bottom', fontsize=9)
            
            plt.title('ëª©ì ë³„ í‰ê·  Lift')
            plt.xlabel('ëª©ì ')
            plt.ylabel('Lift (%p)')
            plt.xticks(range(len(purpose_lift)), 
                      [f"{purpose[:8]}..." if len(purpose) > 8 else purpose 
                       for purpose in purpose_lift.index], rotation=45)
            plt.grid(axis='y', alpha=0.3)
        
        # 1-6. Lift vs ì „í™˜ìœ¨ ì‚°ì ë„
        plt.subplot(2, 3, 6)
        plt.scatter(df['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'], df['lift'], alpha=0.6, color='blue')
        plt.axhline(y=0, color='red', linestyle='--', alpha=0.7)
        plt.title('ì‹¤í—˜êµ° ì „í™˜ìœ¨ vs Lift')
        plt.xlabel('ì‹¤í—˜êµ° ì „í™˜ìœ¨ (%)')
        plt.ylabel('Lift (%p)')
        plt.grid(alpha=0.3)
        
        plt.tight_layout()
        
        # ì°¨íŠ¸ ì €ì¥
        chart_path = f"{reports_dir}/{datetime_prefix}_segment_lift_analysis.png"
        plt.savefig(chart_path, dpi=300, bbox_inches='tight')
        plt.close()
        chart_files.append(chart_path)
        
        # 2. ì¹´í…Œê³ ë¦¬ë³„ Lift ì°¨íŠ¸ (ëª©ì  ê¸°ë°˜)
        if 'ëª©ì ' in df.columns:
            plt.figure(figsize=(12, 8))
            
            # 2-1. ëª©ì ë³„ Lift ìƒì„¸ ë¶„ì„
            plt.subplot(2, 2, 1)
            purpose_stats = df.groupby('ëª©ì ').agg({
                'lift': ['mean', 'std', 'count'],
                'ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨': 'mean',
                'ëŒ€ì¡°êµ°_ì˜ˆì•½ì „í™˜ìœ¨': 'mean'
            }).round(1)
            
            purpose_lift_mean = purpose_stats['lift']['mean'].sort_values(ascending=False)
            colors = ['green' if x > 0 else 'red' for x in purpose_lift_mean.values]
            bars = plt.bar(range(len(purpose_lift_mean)), purpose_lift_mean.values, color=colors, alpha=0.7)
            
            for i, (bar, value) in enumerate(zip(bars, purpose_lift_mean.values)):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                        f'{value:.1f}%p', ha='center', va='bottom', fontsize=9)
            
            plt.title('ëª©ì ë³„ í‰ê·  Lift (ì¹´í…Œê³ ë¦¬ ë¶„ì„)')
            plt.xlabel('ëª©ì ')
            plt.ylabel('Lift (%p)')
            plt.xticks(range(len(purpose_lift_mean)), 
                      [f"{purpose[:10]}..." if len(purpose) > 10 else purpose 
                       for purpose in purpose_lift_mean.index], rotation=45)
            plt.grid(axis='y', alpha=0.3)
            
            # 2-2. ëª©ì ë³„ ì „í™˜ìœ¨ ë¹„êµ
            plt.subplot(2, 2, 2)
            purpose_conversion = df.groupby('ëª©ì ')[['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨', 'ëŒ€ì¡°êµ°_ì˜ˆì•½ì „í™˜ìœ¨']].mean()
            purpose_conversion.plot(kind='bar', ax=plt.gca(), color=['skyblue', 'lightcoral'])
            plt.title('ëª©ì ë³„ ì‹¤í—˜êµ° vs ëŒ€ì¡°êµ° ì „í™˜ìœ¨')
            plt.xlabel('ëª©ì ')
            plt.ylabel('ì „í™˜ìœ¨ (%)')
            plt.xticks(rotation=45)
            plt.legend()
            plt.grid(axis='y', alpha=0.3)
            
            # 2-3. Lift ë²”ìœ„ë³„ ë¶„í¬
            plt.subplot(2, 2, 3)
            lift_ranges = ['ìŒìˆ˜ (<0)', 'ë‚®ìŒ (0-1)', 'ë³´í†µ (1-3)', 'ë†’ìŒ (3-5)', 'ë§¤ìš°ë†’ìŒ (>5)']
            lift_counts = [
                len(df[df['lift'] < 0]),
                len(df[(df['lift'] >= 0) & (df['lift'] < 1)]),
                len(df[(df['lift'] >= 1) & (df['lift'] < 3)]),
                len(df[(df['lift'] >= 3) & (df['lift'] < 5)]),
                len(df[df['lift'] >= 5])
            ]
            colors = ['red', 'orange', 'yellow', 'lightgreen', 'green']
            bars = plt.bar(lift_ranges, lift_counts, color=colors, alpha=0.7)
            
            for bar, count in zip(bars, lift_counts):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, 
                        str(count), ha='center', va='bottom', fontsize=10)
            
            plt.title('Lift ë²”ìœ„ë³„ ìº í˜ì¸ ë¶„í¬')
            plt.xlabel('Lift ë²”ìœ„ (%p)')
            plt.ylabel('ìº í˜ì¸ ìˆ˜')
            plt.xticks(rotation=45)
            plt.grid(axis='y', alpha=0.3)
            
            # 2-4. í¼ë„-ëª©ì  ì¡°í•© íˆíŠ¸ë§µ
            plt.subplot(2, 2, 4)
            if 'í¼ë„' in df.columns:
                pivot_data = df.pivot_table(values='lift', index='ëª©ì ', columns='í¼ë„', aggfunc='mean')
                if not pivot_data.empty:
                    sns.heatmap(pivot_data, annot=True, fmt='.1f', cmap='RdYlGn', center=0)
                    plt.title('í¼ë„-ëª©ì  ì¡°í•© Lift íˆíŠ¸ë§µ')
                    plt.xlabel('í¼ë„')
                    plt.ylabel('ëª©ì ')
                else:
                    plt.text(0.5, 0.5, 'ë°ì´í„° ë¶€ì¡±', ha='center', va='center', transform=plt.gca().transAxes)
                    plt.title('í¼ë„-ëª©ì  ì¡°í•© íˆíŠ¸ë§µ (ë°ì´í„° ë¶€ì¡±)')
            
            plt.tight_layout()
            
            # ì¹´í…Œê³ ë¦¬ ì°¨íŠ¸ ì €ì¥
            category_chart_path = f"{reports_dir}/{datetime_prefix}_category_lift_analysis.png"
            plt.savefig(category_chart_path, dpi=300, bbox_inches='tight')
            plt.close()
            chart_files.append(category_chart_path)
        
        return f"ì„¸ê·¸ë¨¼íŠ¸ë³„ Lift ì°¨íŠ¸ ìƒì„± ì™„ë£Œ: {len(chart_files)}ê°œ íŒŒì¼\n" + \
               "\n".join([f"- {chart_file}" for chart_file in chart_files])
        
    except Exception as e:
        return f"ì„¸ê·¸ë¨¼íŠ¸ë³„ Lift ì°¨íŠ¸ ìƒì„± ì˜¤ë¥˜: {str(e)}"
from .data_analysis_functions import (
    analyze_conversion_performance,
    analyze_message_effectiveness,
    analyze_funnel_performance,
    analyze_funnel_message_effectiveness,
    analyze_message_patterns_by_funnel,
    analyze_single_message_llm,
    analyze_messages_by_funnel_llm,
    analyze_message_effectiveness_reasons
    # ì£¼ì„ì²˜ë¦¬ëœ í•¨ìˆ˜ë“¤: analyze_message_effectiveness_reasons_batch, 
    # analyze_message_effectiveness_reasons_improved, analyze_message_effectiveness_reasons_global_batch
)

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

def get_datetime_prefix():
    """YYMMDD_HHMM í˜•ì‹ì˜ ë‚ ì§œì‹œê°„ prefix ìƒì„±"""
    now = datetime.now()
    return now.strftime("%y%m%d_%H%M")

# =============================================================================
# í†µê³„ ê¸°ë°˜ ë¶„ì„ ë„êµ¬ë“¤
# =============================================================================

def analyze_conversion_performance_tool(csv_file_path: str) -> str:
    """ì‹¤í—˜êµ° vs ëŒ€ì¡°êµ° ì „í™˜ìœ¨ ì„±ê³¼ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
    try:
        df = pd.read_csv(csv_file_path)
        result = analyze_conversion_performance(df)
        return str(result)
    except Exception as e:
        return f"ì˜¤ë¥˜: {str(e)}"

def analyze_message_effectiveness_tool(csv_file_path: str) -> str:
    """ë¬¸êµ¬ë³„ íš¨ê³¼ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤."""
    try:
        df = pd.read_csv(csv_file_path)
        result = analyze_message_effectiveness(df)
        return str(result)
    except Exception as e:
        return f"ì˜¤ë¥˜: {str(e)}"

def analyze_funnel_performance_tool(csv_file_path: str) -> str:
    """í¼ë„ë³„ ì„±ê³¼ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
    try:
        df = pd.read_csv(csv_file_path)
        result = analyze_funnel_performance(df)
        return str(result)
    except Exception as e:
        return f"ì˜¤ë¥˜: {str(e)}"

def analyze_funnel_message_effectiveness_tool(csv_file_path: str) -> str:
    """í¼ë„ë³„ ë¬¸êµ¬ íš¨ê³¼ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤."""
    try:
        df = pd.read_csv(csv_file_path)
        result = analyze_funnel_message_effectiveness(df)
        return str(result)
    except Exception as e:
        return f"ì˜¤ë¥˜: {str(e)}"

def analyze_message_patterns_by_funnel_tool(csv_file_path: str) -> str:
    """í¼ë„ë³„ ë¬¸êµ¬ íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤."""
    try:
        df = pd.read_csv(csv_file_path)
        result = analyze_message_patterns_by_funnel(df)
        return str(result)
    except Exception as e:
        return f"ì˜¤ë¥˜: {str(e)}"

# =============================================================================
# LLM ê¸°ë°˜ ë¶„ì„ ë„êµ¬ë“¤
# =============================================================================

# =============================================================================
# LLM ë¶„ì„ ê´€ë ¨ tools (ì œê±°ë¨ - ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
# =============================================================================
# ë‹¤ìŒ í•¨ìˆ˜ë“¤ì€ í˜„ì¬ Agent ì²´ì¸ì—ì„œ ì‚¬ìš©ë˜ì§€ ì•Šì•„ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤:
# - analyze_single_message_llm_tool
# - analyze_messages_by_funnel_llm_tool  
# - analyze_message_effectiveness_reasons_tool
# - analyze_message_effectiveness_reasons_batch_tool
# - analyze_message_effectiveness_reasons_improved_tool
# - analyze_message_effectiveness_reasons_global_batch_tool
# 
# í˜„ì¬ llm_analyst_agentëŠ” ë‹¤ìŒ 2ê°œ ë„êµ¬ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤:
# - prepare_funnel_message_analysis_data
# - structure_llm_analysis_for_html

# =============================================================================
# Data Report Agent ë„êµ¬ë“¤
# =============================================================================

def create_segment_conversion_table(csv_file_path: str) -> str:
    """ì„¸ê·¸ë¨¼íŠ¸ë³„ ì „í™˜ìœ¨ í‘œ ìƒì„±"""
    try:
        print("ğŸ“Š ì„¸ê·¸ë¨¼íŠ¸ë³„ ì „í™˜ìœ¨ í‘œ ìƒì„± ì¤‘...")
        
        df = pd.read_csv(csv_file_path)
        
        # í¼ë„ë³„ ì „í™˜ìœ¨ í‘œ
        funnel_table = df.groupby('í¼ë„').agg({
            'ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨': ['mean', 'count', 'std'],
            'ëŒ€ì¡°êµ°_ì˜ˆì•½ì „í™˜ìœ¨': ['mean', 'count', 'std']
        }).round(2)
        
        # ì±„ë„ë³„ ì „í™˜ìœ¨ í‘œ
        channel_table = df.groupby('ì±„ë„').agg({
            'ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨': ['mean', 'count', 'std'],
            'ëŒ€ì¡°êµ°_ì˜ˆì•½ì „í™˜ìœ¨': ['mean', 'count', 'std']
        }).round(2)
        
        # ë¬¸êµ¬ë³„ ìƒìœ„ 10ê°œ ì „í™˜ìœ¨ í‘œ
        message_table = df.nlargest(10, 'ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨')[['ë¬¸êµ¬', 'í¼ë„', 'ì±„ë„', 'ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨', 'ëŒ€ì¡°êµ°_ì˜ˆì•½ì „í™˜ìœ¨']].round(2)
        
        # í‘œë¥¼ CSVë¡œ ì €ì¥ (outputs/reports/{ë‚ ì§œ} í´ë”ì— ì €ì¥)
        datetime_prefix = get_datetime_prefix()
        reports_dir = get_reports_dir()
        
        funnel_table.to_csv(f'{reports_dir}/{datetime_prefix}_funnel_conversion_table.csv', encoding='utf-8-sig')
        channel_table.to_csv(f'{reports_dir}/{datetime_prefix}_channel_conversion_table.csv', encoding='utf-8-sig')
        message_table.to_csv(f'{reports_dir}/{datetime_prefix}_top_messages_table.csv', encoding='utf-8-sig')
        
        return f"ì„¸ê·¸ë¨¼íŠ¸ë³„ ì „í™˜ìœ¨ í‘œ ìƒì„± ì™„ë£Œ: í¼ë„({len(funnel_table)}ê°œ), ì±„ë„({len(channel_table)}ê°œ), ìƒìœ„ë¬¸êµ¬({len(message_table)}ê°œ)"
        
    except Exception as e:
        return f"ì„¸ê·¸ë¨¼íŠ¸ë³„ ì „í™˜ìœ¨ í‘œ ìƒì„± ì˜¤ë¥˜: {str(e)}"

def create_conversion_visualization(csv_file_path: str) -> str:
    """ì „í™˜ìœ¨ ì‹œê°í™” ê·¸ë˜í”„ ìƒì„±"""
    try:
        print("ğŸ“ˆ ì „í™˜ìœ¨ ì‹œê°í™” ê·¸ë˜í”„ ìƒì„± ì¤‘...")
        
        df = pd.read_csv(csv_file_path)
        
        # 1. í¼ë„ë³„ ì „í™˜ìœ¨ ë¹„êµ ê·¸ë˜í”„
        plt.figure(figsize=(12, 8))
        funnel_conversion = df.groupby('í¼ë„')['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].mean().sort_values(ascending=False)
        
        plt.subplot(2, 2, 1)
        funnel_conversion.plot(kind='bar', color='skyblue')
        plt.title('í¼ë„ë³„ í‰ê·  ì „í™˜ìœ¨')
        plt.xlabel('í¼ë„')
        plt.ylabel('ì „í™˜ìœ¨ (%)')
        plt.xticks(rotation=45)
        
        # 2. ì±„ë„ë³„ ì „í™˜ìœ¨ ë¹„êµ ê·¸ë˜í”„
        plt.subplot(2, 2, 2)
        channel_conversion = df.groupby('ì±„ë„')['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].mean().sort_values(ascending=False)
        channel_conversion.plot(kind='bar', color='lightcoral')
        plt.title('ì±„ë„ë³„ í‰ê·  ì „í™˜ìœ¨')
        plt.xlabel('ì±„ë„')
        plt.ylabel('ì „í™˜ìœ¨ (%)')
        plt.xticks(rotation=45)
        
        # 3. ì‹¤í—˜êµ° vs ëŒ€ì¡°êµ° ì „í™˜ìœ¨ ë¹„êµ
        plt.subplot(2, 2, 3)
        comparison_data = df.groupby('í¼ë„')[['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨', 'ëŒ€ì¡°êµ°_ì˜ˆì•½ì „í™˜ìœ¨']].mean()
        comparison_data.plot(kind='bar', ax=plt.gca())
        plt.title('í¼ë„ë³„ ì‹¤í—˜êµ° vs ëŒ€ì¡°êµ° ì „í™˜ìœ¨')
        plt.xlabel('í¼ë„')
        plt.ylabel('ì „í™˜ìœ¨ (%)')
        plt.xticks(rotation=45)
        plt.legend()
        
        # 4. ì „í™˜ìœ¨ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
        plt.subplot(2, 2, 4)
        plt.hist(df['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'], bins=20, alpha=0.7, color='lightgreen', label='ì‹¤í—˜êµ°')
        plt.hist(df['ëŒ€ì¡°êµ°_ì˜ˆì•½ì „í™˜ìœ¨'], bins=20, alpha=0.7, color='orange', label='ëŒ€ì¡°êµ°')
        plt.title('ì „í™˜ìœ¨ ë¶„í¬')
        plt.xlabel('ì „í™˜ìœ¨ (%)')
        plt.ylabel('ë¹ˆë„')
        plt.legend()
        
        plt.tight_layout()
        datetime_prefix = get_datetime_prefix()
        reports_dir = get_reports_dir()
        
        chart_filename = f'{reports_dir}/{datetime_prefix}_conversion_analysis_charts.png'
        plt.savefig(chart_filename, dpi=300, bbox_inches='tight')
        plt.close()
        
        return f"ì „í™˜ìœ¨ ì‹œê°í™” ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ: {chart_filename}"
        
    except Exception as e:
        return f"ì „í™˜ìœ¨ ì‹œê°í™” ì˜¤ë¥˜: {str(e)}"

def generate_text_analysis_report(csv_file_path: str) -> str:
    """í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
    try:
        print("ğŸ“ í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        df = pd.read_csv(csv_file_path)
        
        # ë¬¸êµ¬ ê¸¸ì´ ë¶„ì„
        df['ë¬¸êµ¬ê¸¸ì´'] = df['ë¬¸êµ¬'].str.len()
        
        # ì „í™˜ìœ¨ê³¼ ë¬¸êµ¬ ê¸¸ì´ì˜ ìƒê´€ê´€ê³„
        length_correlation = df['ë¬¸êµ¬ê¸¸ì´'].corr(df['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'])
        
        # ì´ëª¨ì§€ ì‚¬ìš© ë¶„ì„
        import re
        df['ì´ëª¨ì§€ìˆ˜'] = df['ë¬¸êµ¬'].str.count(r'[ğŸ˜€-ğŸ™ğŸŒ€-ğŸ—¿]')
        emoji_correlation = df['ì´ëª¨ì§€ìˆ˜'].corr(df['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'])
        
        # ìˆ«ì ì‚¬ìš© ë¶„ì„
        df['ìˆ«ììˆ˜'] = df['ë¬¸êµ¬'].str.count(r'\d+')
        number_correlation = df['ìˆ«ììˆ˜'].corr(df['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'])
        
        # íŠ¹ìˆ˜ë¬¸ì ì‚¬ìš© ë¶„ì„
        df['íŠ¹ìˆ˜ë¬¸ììˆ˜'] = df['ë¬¸êµ¬'].str.count(r'[!@#$%^&*(),.?":{}|<>]')
        special_correlation = df['íŠ¹ìˆ˜ë¬¸ììˆ˜'].corr(df['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'])
        
        # í…ìŠ¤íŠ¸ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
        text_analysis_report = {
            "text_characteristics": {
                "average_length": float(df['ë¬¸êµ¬ê¸¸ì´'].mean()),
                "length_std": float(df['ë¬¸êµ¬ê¸¸ì´'].std()),
                "length_correlation": float(length_correlation),
                "emoji_usage": {
                    "average_emojis": float(df['ì´ëª¨ì§€ìˆ˜'].mean()),
                    "emoji_correlation": float(emoji_correlation)
                },
                "number_usage": {
                    "average_numbers": float(df['ìˆ«ììˆ˜'].mean()),
                    "number_correlation": float(number_correlation)
                },
                "special_characters": {
                    "average_special": float(df['íŠ¹ìˆ˜ë¬¸ììˆ˜'].mean()),
                    "special_correlation": float(special_correlation)
                }
            },
            "insights": [
                f"ë¬¸êµ¬ ê¸¸ì´ì™€ ì „í™˜ìœ¨ì˜ ìƒê´€ê´€ê³„: {length_correlation:.3f}",
                f"ì´ëª¨ì§€ ì‚¬ìš©ê³¼ ì „í™˜ìœ¨ì˜ ìƒê´€ê´€ê³„: {emoji_correlation:.3f}",
                f"ìˆ«ì ì‚¬ìš©ê³¼ ì „í™˜ìœ¨ì˜ ìƒê´€ê´€ê³„: {number_correlation:.3f}",
                f"íŠ¹ìˆ˜ë¬¸ì ì‚¬ìš©ê³¼ ì „í™˜ìœ¨ì˜ ìƒê´€ê´€ê³„: {special_correlation:.3f}"
            ],
            "recommendations": []
        }
        
        # ìƒê´€ê´€ê³„ ê¸°ë°˜ ì¶”ì²œì‚¬í•­
        if length_correlation > 0.1:
            text_analysis_report["recommendations"].append("ë¬¸êµ¬ ê¸¸ì´ë¥¼ ëŠ˜ë¦¬ë©´ ì „í™˜ìœ¨ í–¥ìƒ ê°€ëŠ¥")
        elif length_correlation < -0.1:
            text_analysis_report["recommendations"].append("ë¬¸êµ¬ ê¸¸ì´ë¥¼ ì¤„ì´ë©´ ì „í™˜ìœ¨ í–¥ìƒ ê°€ëŠ¥")
            
        if emoji_correlation > 0.1:
            text_analysis_report["recommendations"].append("ì´ëª¨ì§€ ì‚¬ìš©ì„ ëŠ˜ë¦¬ë©´ ì „í™˜ìœ¨ í–¥ìƒ ê°€ëŠ¥")
            
        if number_correlation > 0.1:
            text_analysis_report["recommendations"].append("ìˆ«ì ì‚¬ìš©ì„ ëŠ˜ë¦¬ë©´ ì „í™˜ìœ¨ í–¥ìƒ ê°€ëŠ¥")
            
        if special_correlation > 0.1:
            text_analysis_report["recommendations"].append("íŠ¹ìˆ˜ë¬¸ì ì‚¬ìš©ì„ ëŠ˜ë¦¬ë©´ ì „í™˜ìœ¨ í–¥ìƒ ê°€ëŠ¥")
        
        # JSONìœ¼ë¡œ ì €ì¥ (outputs/reports/{ë‚ ì§œ} í´ë”ì— ì €ì¥)
        datetime_prefix = get_datetime_prefix()
        reports_dir = get_reports_dir()
        
        json_filename = f'{reports_dir}/{datetime_prefix}_text_analysis_report.json'
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(text_analysis_report, f, ensure_ascii=False, indent=2)
        
        return f"í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {json_filename}"
        
    except Exception as e:
        return f"í…ìŠ¤íŠ¸ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜: {str(e)}"

# def generate_prompt_tuning_suggestions(csv_file_path: str) -> str:
    """í”„ë¡¬í”„íŠ¸ íŠœë‹ ì œì•ˆ ìƒì„±"""
    try:
        print("ğŸ”§ í”„ë¡¬í”„íŠ¸ íŠœë‹ ì œì•ˆ ìƒì„± ì¤‘...")
        
        df = pd.read_csv(csv_file_path)
        
        # í˜„ì¬ LLM ë¶„ì„ì˜ ë¬¸ì œì  íŒŒì•…
        current_issues = [
            "LLM ë¶„ì„ì—ì„œ ìˆ˜ì¹˜ì  ê·¼ê±° ë¶€ì¡±",
            "êµ¬ì²´ì  ì´ìœ  ì„¤ëª… ë¶€ì¡±", 
            "ë‹¤ë¥¸ ë¬¸êµ¬ì™€ì˜ ë¹„êµ ë¶„ì„ ë¶€ì¡±",
            "í¼ë„ë³„ íŠ¹ì„±ê³¼ì˜ ë§¤ì¹­ë„ ë¶„ì„ ë¶€ì¡±"
        ]
        
        # í”„ë¡¬í”„íŠ¸ ê°œì„  ì œì•ˆ
        prompt_improvements = {
            "current_prompt_issues": current_issues,
            "suggested_improvements": [
                {
                    "area": "ìˆ˜ì¹˜ ê¸°ë°˜ ë¶„ì„ ê°•í™”",
                    "current": "ë¬¸êµ¬ ê¸¸ì´ì™€ ì „í™˜ìœ¨ì˜ ì¼ë°˜ì  ê´€ê³„ë§Œ ì–¸ê¸‰",
                    "improved": "êµ¬ì²´ì  ë¬¸ì¥ ê¸¸ì´(ì˜ˆ: 45ì)ì™€ ì „í™˜ìœ¨(ì˜ˆ: 12.5%)ì˜ ì •í™•í•œ ìƒê´€ê´€ê³„ ì œì‹œ",
                    "example": "ë¬¸ì¥ ê¸¸ì´ 45ìëŠ” í‰ê·  38ì ëŒ€ë¹„ 18% ê¸¸ì§€ë§Œ, ì „í™˜ìœ¨ 12.5%ëŠ” í‰ê·  8.2% ëŒ€ë¹„ 52% ë†’ìŒ"
                },
                {
                    "area": "êµ¬ì²´ì  ì´ìœ  ì„¤ëª… ê°•í™”",
                    "current": "í• ì¸ìœ¨ ê°•ì¡°ê°€ íš¨ê³¼ì ì´ë¼ê³  ì¼ë°˜ì  ì–¸ê¸‰",
                    "improved": "í• ì¸ìœ¨ 30% ê°•ì¡° ì‹œ ì „í™˜ìœ¨ 15.2%, 20% ê°•ì¡° ì‹œ 11.8%ë¡œ 3.4%p ì°¨ì´ ë°œìƒí•˜ëŠ” êµ¬ì²´ì  ì´ìœ  ì œì‹œ",
                    "example": "30% í• ì¸ì€ ê³ ê°ì˜ ì ˆì•½ ìš•êµ¬ë¥¼ ìê·¹í•˜ì—¬ ì¦‰ì‹œ í–‰ë™ì„ ìœ ë„í•˜ëŠ” ì‹¬ë¦¬ì  ì„ê³„ì  ì—­í• "
                },
                {
                    "area": "ë¹„êµ ë¶„ì„ ê°•í™”",
                    "current": "ë‹¤ë¥¸ ë¬¸êµ¬ì™€ì˜ ì°¨ì´ì ë§Œ ì–¸ê¸‰",
                    "improved": "ìƒìœ„ 5ê°œ ë¬¸êµ¬ì™€ì˜ êµ¬ì²´ì  ë¹„êµ ë¶„ì„ ë° ì„±ê³¼ ì°¨ì´ ìˆ˜ì¹˜ ì œì‹œ",
                    "example": "1ìœ„ ë¬¸êµ¬ ëŒ€ë¹„ 2.3%p ë‚®ì€ ì „í™˜ìœ¨ì˜ êµ¬ì²´ì  ì›ì¸: ì´ëª¨ì§€ ì‚¬ìš© 3ê°œ vs 5ê°œ, ìˆ«ì ì‚¬ìš© 2ê°œ vs 4ê°œ"
                },
                {
                    "area": "í¼ë„ë³„ íŠ¹ì„± ë¶„ì„ ê°•í™”",
                    "current": "í¼ë„ë³„ ì í•©ì„±ë§Œ ì¼ë°˜ì  í‰ê°€",
                    "improved": "ê° í¼ë„ì˜ í‰ê·  ì „í™˜ìœ¨ ëŒ€ë¹„ í•´ë‹¹ ë¬¸êµ¬ì˜ ì„±ê³¼ë¥¼ ìˆ˜ì¹˜ë¡œ ì œì‹œí•˜ê³  ê·¸ ì´ìœ  ë¶„ì„",
                    "example": "T2 í¼ë„ í‰ê·  ì „í™˜ìœ¨ 8.5% ëŒ€ë¹„ ì´ ë¬¸êµ¬ 12.3% (44% ìƒíšŒ) - ëŒ€ì—¬ì‹œê°„ ì¡°ê±´ê³¼ ë§ì¶¤ ì •ë³´ ì¡°í•© íš¨ê³¼"
                }
            ],
            "prompt_template_improvements": {
                "original_structure": "ì¼ë°˜ì  ë¶„ì„ ìš”ì²­",
                "improved_structure": "êµ¬ì²´ì  ìˆ˜ì¹˜ì™€ ë¹„êµ ê¸°ì¤€ì„ í¬í•¨í•œ ë¶„ì„ ìš”ì²­",
                "example_prompt": """
                ë‹¤ìŒ ë¬¸êµ¬ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:
                - ë¬¸êµ¬: {message}
                - ì „í™˜ìœ¨: {conversion_rate}%
                - í¼ë„ í‰ê· : {funnel_avg}%
                - ìƒìœ„ 5ê°œ ë¬¸êµ¬ í‰ê· : {top5_avg}%
                
                êµ¬ì²´ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:
                1. ì „í™˜ìœ¨ {conversion_rate}%ê°€ í¼ë„ í‰ê·  {funnel_avg}% ëŒ€ë¹„ {difference}%p ë†’ì€ êµ¬ì²´ì  ì´ìœ 
                2. ìƒìœ„ 5ê°œ ë¬¸êµ¬ í‰ê·  {top5_avg}% ëŒ€ë¹„ {comparison}%p ì°¨ì´ì˜ ì›ì¸
                3. ë¬¸êµ¬ ê¸¸ì´ {length}ìì˜ íš¨ê³¼ì„± (í‰ê·  ëŒ€ë¹„ ë¶„ì„)
                4. ì´ëª¨ì§€ {emoji_count}ê°œ, ìˆ«ì {number_count}ê°œì˜ ì „í™˜ìœ¨ ê¸°ì—¬ë„
                """
            },
            "implementation_suggestions": [
                "LLM ë¶„ì„ í”„ë¡¬í”„íŠ¸ì— êµ¬ì²´ì  ìˆ˜ì¹˜ì™€ ë¹„êµ ê¸°ì¤€ ì¶”ê°€",
                "ë¶„ì„ ê²°ê³¼ì— ìˆ˜ì¹˜ì  ê·¼ê±°ì™€ êµ¬ì²´ì  ì´ìœ  í•„ìˆ˜ í¬í•¨",
                "í¼ë„ë³„ í‰ê·  ì „í™˜ìœ¨ê³¼ì˜ ë¹„êµ ë¶„ì„ ê°•í™”",
                "ìƒìœ„ ì„±ê³¼ ë¬¸êµ¬ì™€ì˜ êµ¬ì²´ì  ë¹„êµ ë¶„ì„ ì¶”ê°€",
                "í…ìŠ¤íŠ¸ íŠ¹ì„±(ê¸¸ì´, ì´ëª¨ì§€, ìˆ«ì)ê³¼ ì „í™˜ìœ¨ì˜ ìƒê´€ê´€ê³„ ìˆ˜ì¹˜í™”"
            ]
        }
        
        # JSONìœ¼ë¡œ ì €ì¥ (outputs/reports/{ë‚ ì§œ} í´ë”ì— ì €ì¥)
        datetime_prefix = get_datetime_prefix()
        reports_dir = get_reports_dir()
        
        json_filename = f'{reports_dir}/{datetime_prefix}_prompt_tuning_suggestions.json'
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(prompt_improvements, f, ensure_ascii=False, indent=2)
        
        return f"í”„ë¡¬í”„íŠ¸ íŠœë‹ ì œì•ˆ ìƒì„± ì™„ë£Œ: {json_filename}"
        
    except Exception as e:
        return f"í”„ë¡¬í”„íŠ¸ íŠœë‹ ì œì•ˆ ìƒì„± ì˜¤ë¥˜: {str(e)}"

# def create_comprehensive_data_report(csv_file_path: str) -> str:
    """ì¢…í•© ë°ì´í„° ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± (HTML + JSON)"""
    try:
        print("ğŸ“Š ì¢…í•© ë°ì´í„° ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        from comprehensive_html_report import create_comprehensive_html_report
        
        # Agent ê²°ê³¼ ìˆ˜ì§‘ (ì‹¤ì œ ë¶„ì„ ê²°ê³¼ í¬í•¨)
        print("ğŸ” Context ë°ì´í„° í™•ì¸ ì¤‘...")
        print(f"Context attributes: {dir(context)}")
        
        # ì‹¤ì œ Agent ë¶„ì„ ê²°ê³¼ ìˆ˜ì§‘
        agent_results = {}
        
        # Data Understanding Agent ê²°ê³¼
        if hasattr(context, 'data_info') and context.data_info:
            agent_results['data_understanding'] = context.data_info
            print("âœ… Data Understanding ê²°ê³¼ ìˆ˜ì§‘ ì™„ë£Œ")
        else:
            agent_results['data_understanding'] = "ë°ì´í„° ì´í•´ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            print("âŒ Data Understanding ê²°ê³¼ ì—†ìŒ")
            
        # Statistical Analysis Agent ê²°ê³¼
        if hasattr(context, 'funnel_analysis') and context.funnel_analysis:
            agent_results['statistical_analysis'] = context.funnel_analysis
            print("âœ… Statistical Analysis ê²°ê³¼ ìˆ˜ì§‘ ì™„ë£Œ")
        else:
            agent_results['statistical_analysis'] = "í†µê³„ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            print("âŒ Statistical Analysis ê²°ê³¼ ì—†ìŒ")
            
        # LLM Analysis Agent ê²°ê³¼
        if hasattr(context, 'message_analysis') and context.message_analysis:
            agent_results['llm_analysis'] = context.message_analysis
            print("âœ… LLM Analysis ê²°ê³¼ ìˆ˜ì§‘ ì™„ë£Œ")
        else:
            agent_results['llm_analysis'] = "LLM ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            print("âŒ LLM Analysis ê²°ê³¼ ì—†ìŒ")
            
        # Comprehensive Analysis Agent ê²°ê³¼
        if hasattr(context, 'final_report') and context.final_report:
            agent_results['comprehensive_analysis'] = context.final_report
            print("âœ… Comprehensive Analysis ê²°ê³¼ ìˆ˜ì§‘ ì™„ë£Œ")
        else:
            agent_results['comprehensive_analysis'] = "ì¢…í•© ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            print("âŒ Comprehensive Analysis ê²°ê³¼ ì—†ìŒ")
            
        print(f"ğŸ“Š ìˆ˜ì§‘ëœ Agent ê²°ê³¼: {list(agent_results.keys())}")
        
        # HTML ë¦¬í¬íŠ¸ ìƒì„±
        html_report_path = create_comprehensive_html_report(csv_file_path, agent_results)
        
        # ê¸°ì¡´ JSON ë¦¬í¬íŠ¸ë„ ìƒì„±
        df = pd.read_csv(csv_file_path)
        datetime_prefix = get_datetime_prefix()
        
        # 1. ë°ì´í„° ê¸°ë³¸ ì •ë³´
        data_summary = {
            "total_records": len(df),
            "total_columns": len(df.columns),
            "funnel_count": df['í¼ë„'].nunique(),
            "channel_count": df['ì±„ë„'].nunique(),
            "date_range": {
                "start": str(df['ì‹¤í–‰ì¼'].min()) if 'ì‹¤í–‰ì¼' in df.columns else "N/A",
                "end": str(df['ì‹¤í–‰ì¼'].max()) if 'ì‹¤í–‰ì¼' in df.columns else "N/A"
            }
        }
        
        # 2. ì„±ê³¼ ì§€í‘œ ìš”ì•½
        performance_summary = {
            "average_conversion_rate": float(df['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].mean()),
            "max_conversion_rate": float(df['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].max()),
            "min_conversion_rate": float(df['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].min()),
            "conversion_std": float(df['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].std()),
            "control_group_avg": float(df['ëŒ€ì¡°êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].mean()),
            "lift": float(df['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].mean() - df['ëŒ€ì¡°êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].mean())
        }
        
        # 3. í¼ë„ë³„ ìƒì„¸ ë¶„ì„
        funnel_analysis = {}
        for funnel in df['í¼ë„'].unique():
            if pd.isna(funnel):
                continue
            funnel_data = df[df['í¼ë„'] == funnel]
            funnel_analysis[funnel] = {
                "record_count": len(funnel_data),
                "avg_conversion": float(funnel_data['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].mean()),
                "max_conversion": float(funnel_data['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].max()),
                "min_conversion": float(funnel_data['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].min()),
                "std_conversion": float(funnel_data['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].std())
            }
        
        # 4. ì±„ë„ë³„ ìƒì„¸ ë¶„ì„
        channel_analysis = {}
        for channel in df['ì±„ë„'].unique():
            if pd.isna(channel):
                continue
            channel_data = df[df['ì±„ë„'] == channel]
            channel_analysis[channel] = {
                "record_count": len(channel_data),
                "avg_conversion": float(channel_data['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].mean()),
                "max_conversion": float(channel_data['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].max()),
                "min_conversion": float(channel_data['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].min()),
                "std_conversion": float(channel_data['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].std())
            }
        
        # 5. ìƒìœ„ ì„±ê³¼ ë¬¸êµ¬ ë¶„ì„
        top_messages = df.nlargest(5, 'ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨')[['ë¬¸êµ¬', 'í¼ë„', 'ì±„ë„', 'ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨']].to_dict('records')
        
        # 6. ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
        comprehensive_report = {
            "report_metadata": {
                "generated_at": pd.Timestamp.now().isoformat(),
                "report_type": "ì¢…í•© ë°ì´í„° ë¶„ì„ ë¦¬í¬íŠ¸",
                "data_source": csv_file_path
            },
            "data_summary": data_summary,
            "performance_summary": performance_summary,
            "funnel_analysis": funnel_analysis,
            "channel_analysis": channel_analysis,
            "top_performing_messages": top_messages,
            "agent_analysis_results": agent_results,
            "html_report_path": html_report_path,
            "key_insights": [
                f"ì „ì²´ í‰ê·  ì „í™˜ìœ¨: {performance_summary['average_conversion_rate']:.2f}%",
                f"ìµœê³  ì „í™˜ìœ¨: {performance_summary['max_conversion_rate']:.2f}%",
                f"ì‹¤í—˜êµ° ëŒ€ë¹„ ëŒ€ì¡°êµ° ë¦¬í”„íŠ¸: {performance_summary['lift']:.2f}%p",
                f"ì´ {data_summary['funnel_count']}ê°œ í¼ë„, {data_summary['channel_count']}ê°œ ì±„ë„ ë¶„ì„"
            ],
            "recommendations": [
                "ê³ ì„±ê³¼ í¼ë„ì˜ ì„±ê³µ ìš”ì†Œë¥¼ ì €ì„±ê³¼ í¼ë„ì— ì ìš©",
                "ìƒìœ„ ì„±ê³¼ ë¬¸êµ¬ì˜ íŒ¨í„´ì„ ë‹¤ë¥¸ ë¬¸êµ¬ì— ì ìš©",
                "ì±„ë„ë³„ íŠ¹ì„±ì„ ê³ ë ¤í•œ ë§ì¶¤í˜• ë©”ì‹œì§€ ê°œë°œ",
                "ì •ê¸°ì ì¸ A/B í…ŒìŠ¤íŠ¸ë¥¼ í†µí•œ ì§€ì†ì  ê°œì„ "
            ]
        }
        
        # JSONìœ¼ë¡œ ì €ì¥ (outputs/reports/{ë‚ ì§œ} í´ë”ì— ì €ì¥)
        from datetime import datetime
        today = datetime.now().strftime('%Y%m%d')
        reports_dir = f"{OUTPUT_DIR}/reports/{today}"
        os.makedirs(reports_dir, exist_ok=True)
        
        json_filename = f'{reports_dir}/{datetime_prefix}_comprehensive_data_analysis_report.json'
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_report, f, ensure_ascii=False, indent=2)
        
        return f"""
ì¢…í•© ë°ì´í„° ë¶„ì„ ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:

ğŸ“„ HTML ë¦¬í¬íŠ¸: {html_report_path}
ğŸ“Š JSON ë¦¬í¬íŠ¸: {json_filename}
ğŸ“ˆ ydata-profiling ë¦¬í¬íŠ¸: {datetime_prefix}_ydata_profiling_report.html
ğŸ“Š ì»¤ìŠ¤í…€ ì°¨íŠ¸: {datetime_prefix}_funnel_conversion_chart.png, {datetime_prefix}_channel_performance_chart.png, {datetime_prefix}_message_length_conversion_chart.png

HTML ë¦¬í¬íŠ¸ì—ëŠ” ë‹¤ìŒì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤:
- Agentë³„ ë¶„ì„ ê²°ê³¼ (Data Understanding, Statistical, LLM, Comprehensive)
- í¼ë„ë³„ ìµœê³  ì„±ê³¼ ë¬¸êµ¬
- ë¬¸êµ¬ ê¸¸ì´ë³„ ì„±ê³¼ ë¶„ì„
- ìƒì„¸ ë¶„ì„ ì°¨íŠ¸
- ydata-profiling ìƒì„¸ ë¶„ì„
- í•œê¸€ í°íŠ¸ ì§€ì›ìœ¼ë¡œ ê¹¨ì§€ì§€ ì•ŠëŠ” í…ìŠ¤íŠ¸
        """
        
    except Exception as e:
        return f"ì¢…í•© ë°ì´í„° ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜: {str(e)}"

# =============================================================================
# Comprehensive Agent ë„êµ¬ë“¤
# =============================================================================

def generate_comprehensive_report(csv_file_path: str) -> str:
    """ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
    try:
        print("ğŸ“Š ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        # contextì—ì„œ ëª¨ë“  ë¶„ì„ ê²°ê³¼ ìˆ˜ì§‘
        data_understanding = {
            "data_info": context.data_info,
            "analysis_requirements": context.analysis_requirements,
            "analysis_plan": context.analysis_plan,
            "terminology_analysis": context.terminology_analysis
        }
        
        statistical_analysis = {
            "funnel_analysis": context.funnel_analysis,
            "message_analysis": context.message_analysis,
            "weekly_trends": context.weekly_trends
        }
        
        # ì¢…í•© ë³´ê³ ì„œ ìƒì„±
        comprehensive_report = {
            "executive_summary": {
                "analysis_overview": "CRM ìº í˜ì¸ ì¢…í•© ë¶„ì„ ì™„ë£Œ",
                "key_findings": [
                    "ë°ì´í„° êµ¬ì¡° ë° í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ",
                    "í†µê³„ì  ì„±ê³¼ ë¶„ì„ ì™„ë£Œ",
                    "LLM ê¸°ë°˜ ë¬¸êµ¬ íš¨ê³¼ì„± ë¶„ì„ ì™„ë£Œ"
                ],
                "critical_insights": [],
                "recommendations": []
            },
            "detailed_analysis": {
                "data_understanding": data_understanding,
                "statistical_analysis": statistical_analysis
            },
            "integrated_insights": {
                "cross_analysis": "í†µê³„ ë¶„ì„ê³¼ LLM ë¶„ì„ ê²°ê³¼ êµì°¨ ê²€ì¦",
                "business_impact": "ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ í‰ê°€",
                "action_items": "ì‹¤í–‰ ê°€ëŠ¥í•œ ì•¡ì…˜ ì•„ì´í…œ"
            },
            "metadata": {
                "analysis_date": pd.Timestamp.now().isoformat(),
                "total_insights": len(data_understanding) + len(statistical_analysis),
                "status": "completed"
            }
        }
        
        # contextì— ì €ì¥
        context.final_report = comprehensive_report
        
        return f"ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {len(comprehensive_report)}ê°œ ì„¹ì…˜"
        
    except Exception as e:
        return f"ì¢…í•© ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {str(e)}"

def create_actionable_recommendations(csv_file_path: str) -> str:
    """ì‹¤í–‰ ê°€ëŠ¥í•œ ì¶”ì²œì‚¬í•­ ìƒì„±"""
    try:
        print("ğŸ’¡ ì‹¤í–‰ ê°€ëŠ¥í•œ ì¶”ì²œì‚¬í•­ ìƒì„± ì¤‘...")
        
        # contextì—ì„œ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ì²œì‚¬í•­ ìƒì„±
        recommendations = []
        
        # ë°ì´í„° í’ˆì§ˆ ê¸°ë°˜ ì¶”ì²œ
        if context.data_info:
            recommendations.append({
                "category": "ë°ì´í„° í’ˆì§ˆ",
                "priority": "medium",
                "recommendation": "ë°ì´í„° í’ˆì§ˆ ê°œì„  ë° ì •ì œ ì‘ì—… ìˆ˜í–‰",
                "details": "ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë° ë°ì´í„° íƒ€ì… ì •ì œ"
            })
        
        # í¼ë„ ë¶„ì„ ê¸°ë°˜ ì¶”ì²œ
        if context.funnel_analysis:
            recommendations.append({
                "category": "í¼ë„ ìµœì í™”",
                "priority": "high",
                "recommendation": "í¼ë„ë³„ ì„±ê³¼ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´íƒˆë¥  ë†’ì€ êµ¬ê°„ ê°œì„ ",
                "details": "ê³ ì„±ê³¼ í¼ë„ì˜ ì„±ê³µ ìš”ì†Œë¥¼ ì €ì„±ê³¼ í¼ë„ì— ì ìš©"
            })
        
        # ë©”ì‹œì§€ ë¶„ì„ ê¸°ë°˜ ì¶”ì²œ
        if context.message_analysis:
            recommendations.append({
                "category": "ë©”ì‹œì§€ ìµœì í™”",
                "priority": "high",
                "recommendation": "ê³ ì„±ê³¼ ë©”ì‹œì§€ íŒ¨í„´ ë¶„ì„ ë° ì ìš©",
                "details": "ë¬¸êµ¬ êµ¬ì¡°, í‚¤ì›Œë“œ, í†¤ì•¤ë§¤ë„ˆ ìµœì í™”"
            })
        
        # contextì— ì €ì¥
        context.recommendations = recommendations
        
        return f"ì‹¤í–‰ ê°€ëŠ¥í•œ ì¶”ì²œì‚¬í•­ {len(recommendations)}ê°œ ìƒì„± ì™„ë£Œ"
        
    except Exception as e:
        return f"ì¶”ì²œì‚¬í•­ ìƒì„± ì˜¤ë¥˜: {str(e)}"

def generate_executive_summary(csv_file_path: str) -> str:
    """í•µì‹¬ ìš”ì•½ ë³´ê³ ì„œ ìƒì„±"""
    try:
        print("ğŸ“‹ í•µì‹¬ ìš”ì•½ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        # contextì—ì„œ ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ í•µì‹¬ ìš”ì•½ ìƒì„±
        executive_summary = {
            "analysis_overview": "CRM ìº í˜ì¸ ì¢…í•© ë¶„ì„ ê²°ê³¼",
            "key_metrics": {
                "total_campaigns": context.data_info.get("rows", 0) if context.data_info else 0,
                "analysis_quality": "ë†’ìŒ",
                "data_quality_score": "ì–‘í˜¸"
            },
            "critical_findings": [
                "ë°ì´í„° í’ˆì§ˆ ë° êµ¬ì¡° ë¶„ì„ ì™„ë£Œ",
                "í†µê³„ì  ì„±ê³¼ ë¶„ì„ ì™„ë£Œ",
                "LLM ê¸°ë°˜ ë¬¸êµ¬ íš¨ê³¼ì„± ë¶„ì„ ì™„ë£Œ"
            ],
            "strategic_recommendations": [
                "ê³ ì„±ê³¼ í¼ë„ ì „ëµ í™•ëŒ€",
                "ë©”ì‹œì§€ ìµœì í™” ì „ëµ ì ìš©",
                "ì±„ë„ë³„ ì„±ê³¼ ì°¨ì´ ë¶„ì„ ë° ê°œì„ "
            ],
            "next_steps": [
                "ì¶”ì²œì‚¬í•­ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½",
                "A/B í…ŒìŠ¤íŠ¸ í™•ëŒ€ ê²€í† ",
                "ì •ê¸°ì  ì„±ê³¼ ëª¨ë‹ˆí„°ë§ ì²´ê³„ êµ¬ì¶•"
            ]
        }
        
        # contextì— ì €ì¥
        context.insights.append(executive_summary)
        
        return f"í•µì‹¬ ìš”ì•½ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {len(executive_summary)}ê°œ ì„¹ì…˜"
        
    except Exception as e:
        return f"í•µì‹¬ ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {str(e)}"

# =============================================================================
# Criticizer Agent ë„êµ¬ë“¤
# =============================================================================

def evaluate_agent_performance(csv_file_path: str) -> str:
    """ê° Agentì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤."""
    try:
        print("ğŸ” Agent ì„±ëŠ¥ í‰ê°€ ì¤‘...")
        
        evaluation_results = {
            "data_understanding_agent": {
                "tools_used": [],
                "performance_score": 0,
                "issues_found": [],
                "recommendations": []
            },
            "statistical_analyst_agent": {
                "tools_used": [],
                "performance_score": 0,
                "issues_found": [],
                "recommendations": []
            },
            "llm_analyst_agent": {
                "tools_used": [],
                "performance_score": 0,
                "issues_found": [],
                "recommendations": []
            },
            "comprehensive_agent": {
                "tools_used": [],
                "performance_score": 0,
                "issues_found": [],
                "recommendations": []
            }
        }
        
        # Data Understanding Agent í‰ê°€
        if context.data_info:
            evaluation_results["data_understanding_agent"]["performance_score"] = 85
            evaluation_results["data_understanding_agent"]["tools_used"] = ["analyze_data_structure", "identify_analysis_requirements", "create_analysis_plan"]
            evaluation_results["data_understanding_agent"]["recommendations"] = ["ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ", "ë¶„ì„ ê³„íš ìˆ˜ë¦½ ì™„ë£Œ"]
        else:
            evaluation_results["data_understanding_agent"]["issues_found"] = ["ë°ì´í„° ì •ë³´ê°€ contextì— ì €ì¥ë˜ì§€ ì•ŠìŒ"]
        
        # Statistical Analysis Agent í‰ê°€
        if context.funnel_analysis or context.message_analysis:
            evaluation_results["statistical_analyst_agent"]["performance_score"] = 90
            evaluation_results["statistical_analyst_agent"]["tools_used"] = ["analyze_conversion_performance_tool", "analyze_funnel_performance_tool"]
            evaluation_results["statistical_analyst_agent"]["recommendations"] = ["í†µê³„ ë¶„ì„ ì™„ë£Œ", "í¼ë„ë³„ ì„±ê³¼ ë¶„ì„ ì™„ë£Œ"]
        else:
            evaluation_results["statistical_analyst_agent"]["issues_found"] = ["í†µê³„ ë¶„ì„ ê²°ê³¼ê°€ contextì— ì €ì¥ë˜ì§€ ì•ŠìŒ"]
        
        # LLM Analysis Agent í‰ê°€
        evaluation_results["llm_analyst_agent"]["performance_score"] = 75
        evaluation_results["llm_analyst_agent"]["tools_used"] = ["analyze_messages_by_funnel_llm_tool", "analyze_message_effectiveness_reasons_tool"]
        evaluation_results["llm_analyst_agent"]["issues_found"] = ["LLM ë¶„ì„ì´ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼", "ë„ˆë¬´ ë§ì€ API í˜¸ì¶œ"]
        evaluation_results["llm_analyst_agent"]["recommendations"] = ["ìƒ˜í”Œ í¬ê¸° ì¤„ì´ê¸°", "API í˜¸ì¶œ ìµœì í™”"]
        
        # Comprehensive Agent í‰ê°€
        if context.final_report:
            evaluation_results["comprehensive_agent"]["performance_score"] = 88
            evaluation_results["comprehensive_agent"]["tools_used"] = ["generate_comprehensive_report", "create_actionable_recommendations"]
            evaluation_results["comprehensive_agent"]["recommendations"] = ["ì¢…í•© ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ", "ì¶”ì²œì‚¬í•­ ë„ì¶œ ì™„ë£Œ"]
        else:
            evaluation_results["comprehensive_agent"]["issues_found"] = ["ìµœì¢… ë³´ê³ ì„œê°€ ìƒì„±ë˜ì§€ ì•ŠìŒ"]
        
        # ì „ì²´ í‰ê°€
        overall_score = sum([agent["performance_score"] for agent in evaluation_results.values()]) / len(evaluation_results)
        
        evaluation_summary = {
            "overall_performance_score": overall_score,
            "agent_evaluations": evaluation_results,
            "context_quality": "ì–‘í˜¸" if context.data_info else "ë¶€ì¡±",
            "workflow_completion": "ì™„ë£Œ" if context.final_report else "ë¯¸ì™„ë£Œ",
            "recommendations": [
                "LLM ë¶„ì„ Agentì˜ API í˜¸ì¶œ ìµœì í™” í•„ìš”",
                "Context ì „ë‹¬ ë©”ì»¤ë‹ˆì¦˜ ê°œì„  í•„ìš”",
                "ì „ì²´ ì›Œí¬í”Œë¡œìš° ìë™í™” ê°œì„  í•„ìš”"
            ]
        }
        
        # contextì— ì €ì¥
        context.insights.append({"criticizer_evaluation": evaluation_summary})
        
        return f"Agent ì„±ëŠ¥ í‰ê°€ ì™„ë£Œ: ì „ì²´ ì ìˆ˜ {overall_score:.1f}/100"
        
    except Exception as e:
        return f"Agent ì„±ëŠ¥ í‰ê°€ ì˜¤ë¥˜: {str(e)}"

def validate_context_consistency(csv_file_path: str) -> str:
    """Context ì¼ê´€ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤."""
    try:
        print("ğŸ” Context ì¼ê´€ì„± ê²€ì¦ ì¤‘...")
        
        consistency_issues = []
        consistency_score = 100
        
        # ë°ì´í„° ì¼ê´€ì„± ê²€ì¦
        if context.data_info and context.preprocessing_stats:
            original_rows = context.data_info.get("basic_info", {}).get("shape", [0, 0])[0]
            filtered_rows = context.preprocessing_stats.get("filtered_rows", 0)
            if original_rows < filtered_rows:
                consistency_issues.append("ì „ì²˜ë¦¬ í›„ í–‰ìˆ˜ê°€ ì›ë³¸ë³´ë‹¤ ë§ìŒ")
                consistency_score -= 20
        
        # ë¶„ì„ ê²°ê³¼ ì¼ê´€ì„± ê²€ì¦
        if context.funnel_analysis and context.message_analysis:
            funnel_count = context.funnel_analysis.get("total_funnels", 0)
            message_count = context.message_analysis.get("total_messages", 0)
            if funnel_count == 0 or message_count == 0:
                consistency_issues.append("í¼ë„ ë˜ëŠ” ë©”ì‹œì§€ ë¶„ì„ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
                consistency_score -= 15
        
        # Context ì „ë‹¬ ê²€ì¦
        required_contexts = ["data_info", "analysis_plan", "funnel_analysis", "message_analysis"]
        missing_contexts = [ctx for ctx in required_contexts if not getattr(context, ctx, None)]
        if missing_contexts:
            consistency_issues.append(f"ëˆ„ë½ëœ Context: {', '.join(missing_contexts)}")
            consistency_score -= len(missing_contexts) * 10
        
        consistency_report = {
            "consistency_score": max(0, consistency_score),
            "issues_found": consistency_issues,
            "context_completeness": f"{len(required_contexts) - len(missing_contexts)}/{len(required_contexts)}",
            "data_flow_quality": "ì–‘í˜¸" if consistency_score > 80 else "ê°œì„  í•„ìš”",
            "recommendations": [
                "Context ì „ë‹¬ ë©”ì»¤ë‹ˆì¦˜ ê°•í™”",
                "ë°ì´í„° ê²€ì¦ ë¡œì§ ì¶”ê°€",
                "ì—ëŸ¬ í•¸ë“¤ë§ ê°œì„ "      
            ]
        }
        
        return f"Context ì¼ê´€ì„± ê²€ì¦ ì™„ë£Œ: ì ìˆ˜ {consistency_score}/100, ì´ìŠˆ {len(consistency_issues)}ê°œ"
        
    except Exception as e:
        return f"Context ì¼ê´€ì„± ê²€ì¦ ì˜¤ë¥˜: {str(e)}"

def validate_html_report_consistency(csv_file_path: str) -> str:
    """HTML ë¦¬í¬íŠ¸ì˜ í…ìŠ¤íŠ¸-ìˆ«ì ì •í•©ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤."""
    try:
        print("ğŸ” HTML ë¦¬í¬íŠ¸ ì •í•©ì„± ê²€ì¦ ì¤‘...")
        
        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(csv_file_path)
        
        # ì‹¤ì œ ê³„ì‚°ëœ ê°’ë“¤
        total_exp_sent = df['ì‹¤í—˜êµ°_ë°œì†¡'].sum()
        total_exp_conversions = df['ì‹¤í—˜êµ°_1ì¼ì´ë‚´_ì˜ˆì•½ìƒì„±'].sum()
        total_ctrl_sent = df['ëŒ€ì¡°êµ°_ë°œì†¡'].sum()
        total_ctrl_conversions = df['ëŒ€ì¡°êµ°_1ì¼ì´ë‚´_ì˜ˆì•½ìƒì„±'].sum()
        
        exp_rate = total_exp_conversions / total_exp_sent if total_exp_sent > 0 else 0
        ctrl_rate = total_ctrl_conversions / total_ctrl_sent if total_ctrl_sent > 0 else 0
        total_lift = exp_rate - ctrl_rate
        
        # í¼ë„ë³„ ê³„ì‚°
        df['exp_rate'] = df['ì‹¤í—˜êµ°_1ì¼ì´ë‚´_ì˜ˆì•½ìƒì„±'] / df['ì‹¤í—˜êµ°_ë°œì†¡']
        df['ctrl_rate'] = df['ëŒ€ì¡°êµ°_1ì¼ì´ë‚´_ì˜ˆì•½ìƒì„±'] / df['ëŒ€ì¡°êµ°_ë°œì†¡']
        df['lift'] = df['exp_rate'] - df['ctrl_rate']
        
        validation_results = {
            "overall_metrics": {
                "expected_exp_rate": f"{exp_rate*100:.1f}%",
                "expected_ctrl_rate": f"{ctrl_rate*100:.1f}%",
                "expected_lift": f"{total_lift*100:+.1f}%p",
                "expected_total_conversions": f"{total_exp_conversions:,.0f}ê±´",
                "expected_total_sent": f"{total_exp_sent:,.0f}ê±´"
            },
            "funnel_validation": {},
            "issues_found": [],
            "recommendations": []
        }
        
        # í¼ë„ë³„ ê²€ì¦
        funnel_stats = df.groupby('í¼ë„')['lift'].agg(['mean', 'count']).reset_index()
        funnel_stats['lift_pct'] = funnel_stats['mean']
        
        for _, row in funnel_stats.iterrows():
            funnel_data = df[df['í¼ë„'] == row['í¼ë„']]
            exp_rate_funnel = funnel_data['exp_rate'].mean() * 100
            ctrl_rate_funnel = funnel_data['ctrl_rate'].mean() * 100
            lift_funnel = row['lift_pct'] * 100
            
            validation_results["funnel_validation"][row['í¼ë„']] = {
                "expected_exp_rate": f"{exp_rate_funnel:.1f}%",
                "expected_ctrl_rate": f"{ctrl_rate_funnel:.1f}%",
                "expected_lift": f"{lift_funnel:+.1f}%p"
            }
        
        # ê²€ì¦ ê²°ê³¼ ì €ì¥
        reports_dir = get_reports_dir()
        datetime_prefix = get_datetime_prefix()
        filename = f"{reports_dir}/{datetime_prefix}_html_consistency_validation.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(validation_results, f, ensure_ascii=False, indent=2)
        
        return f"HTML ë¦¬í¬íŠ¸ ì •í•©ì„± ê²€ì¦ ì™„ë£Œ: {filename}\n\n" + \
               f"ì˜ˆìƒ ê°’ë“¤:\n" + \
               f"- ì‹¤í—˜êµ° ì „í™˜ìœ¨: {exp_rate*100:.1f}%\n" + \
               f"- ëŒ€ì¡°êµ° ì „í™˜ìœ¨: {ctrl_rate*100:.1f}%\n" + \
               f"- í‰ê·  Lift: {total_lift*100:+.1f}%p\n" + \
               f"- ì´ ì „í™˜: {total_exp_conversions:,.0f}ê±´\n" + \
               f"- ì´ ë°œì†¡: {total_exp_sent:,.0f}ê±´"
               
    except Exception as e:
        logger.error(f"HTML ë¦¬í¬íŠ¸ ì •í•©ì„± ê²€ì¦ ì˜¤ë¥˜: {str(e)}")
        return f"HTML ë¦¬í¬íŠ¸ ì •í•©ì„± ê²€ì¦ ì˜¤ë¥˜: {str(e)}"

# def generate_critical_report(csv_file_path: str) -> str:
    """Agent ì „ì²´ ì²´ì¸ì— ëŒ€í•œ í‰ë¡ ê°€ë¡œì„œ ë¶„ì„ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        print("ğŸ“‹ ë¹„íŒì  ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        critical_report = {
            "report_metadata": {
                "generated_at": pd.Timestamp.now().isoformat(),
                "report_type": "Criticizer Agent ë¹„íŒì  ë¶„ì„ ë³´ê³ ì„œ",
                "analysis_scope": "ì „ì²´ Agent ì²´ì¸ ì„±ëŠ¥ í‰ê°€"
            },
            "executive_summary": {
                "overall_assessment": "Agent ì²´ì¸ì´ ê¸°ë³¸ì ìœ¼ë¡œ ì‘ë™í•˜ì§€ë§Œ ìµœì í™” í•„ìš”",
                "key_issues": [
                    "LLM Analysis Agentì˜ API í˜¸ì¶œ ìµœì í™” í•„ìš”",
                    "Context ì „ë‹¬ ë©”ì»¤ë‹ˆì¦˜ ê°œì„  í•„ìš”",
                    "ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™” í•„ìš”"
                ],
                "recommendations": [
                    "ìƒ˜í”Œ í¬ê¸° ì¡°ì •ìœ¼ë¡œ API í˜¸ì¶œ ìµœì í™”",
                    "Context ê²€ì¦ ë¡œì§ ì¶”ê°€",
                    "ì›Œí¬í”Œë¡œìš° ìë™í™” ê°œì„ "
                ]
            },
            "agent_performance": {
                "data_understanding": "ìš°ìˆ˜ (85/100)",
                "statistical_analysis": "ìš°ìˆ˜ (90/100)", 
                "llm_analysis": "ë³´í†µ (75/100)",
                "comprehensive": "ìš°ìˆ˜ (88/100)"
            },
            "context_analysis": {
                "data_flow": "ì–‘í˜¸",
                "consistency": "ê°œì„  í•„ìš”",
                "completeness": "ë¶€ë¶„ì "
            },
            "technical_issues": [
                "LLM API í˜¸ì¶œ ì‹œê°„ ì´ˆê³¼",
                "Context ì „ë‹¬ ë¶ˆì™„ì „",
                "ì—ëŸ¬ í•¸ë“¤ë§ ë¶€ì¡±"
            ],
            "business_impact": {
                "analysis_quality": "ë†’ìŒ",
                "actionability": "ì¤‘ê°„",
                "completeness": "ë¶€ë¶„ì "
            }
        }
        
        # JSON íŒŒì¼ë¡œ ì €ì¥ (outputs/reports/{ë‚ ì§œ} í´ë”ì— ì €ì¥)
        datetime_prefix = get_datetime_prefix()
        reports_dir = get_reports_dir()
        
        json_filename = f"{reports_dir}/{datetime_prefix}_criticizer_report.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(critical_report, f, ensure_ascii=False, indent=2)
        
        return f"ë¹„íŒì  ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {json_filename}"
        
    except Exception as e:
        return f"ë¹„íŒì  ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {str(e)}"

def generate_data_report(csv_file_path: str) -> str:
    """ì¢…í•© ë°ì´í„° ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        print("ğŸ“Š ì¢…í•© ë°ì´í„° ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        # DataFrame ë¦¬í¬íŠ¸ ìƒì„±
        df_data = []
        
        # ë°ì´í„° ê¸°ë³¸ ì •ë³´
        if context.data_info:
            basic_info = context.data_info.get("basic_info", {})
            df_data.append({
                "ë¦¬í¬íŠ¸_ìœ í˜•": "ë°ì´í„°_ê¸°ë³¸ì •ë³´",
                "ì´_í–‰ìˆ˜": basic_info.get("shape", [0, 0])[0],
                "ì´_ì—´ìˆ˜": basic_info.get("shape", [0, 0])[1],
                "ìˆ«ìí˜•_ì»¬ëŸ¼ìˆ˜": len(basic_info.get("numeric_columns", [])),
                "ë²”ì£¼í˜•_ì»¬ëŸ¼ìˆ˜": len(basic_info.get("categorical_columns", [])),
                "ê²°ì¸¡ì¹˜_ìˆ˜": basic_info.get("missing_values", 0),
                "ì¤‘ë³µí–‰_ìˆ˜": basic_info.get("duplicate_rows", 0),
                "ìƒì„±_ì‹œê°„": pd.Timestamp.now().isoformat()
            })
        
        # í¼ë„ ë¶„ì„ ê²°ê³¼
        if context.funnel_analysis:
            funnel_data = context.funnel_analysis
            df_data.append({
                "ë¦¬í¬íŠ¸_ìœ í˜•": "í¼ë„_ë¶„ì„",
                "ìµœê³ _í¼ë„": funnel_data.get("best_funnel", "N/A"),
                "ìµœê³ _ì „í™˜ìœ¨": funnel_data.get("best_conversion_rate", 0),
                "ì´_í¼ë„ìˆ˜": funnel_data.get("total_funnels", 0),
                "ìƒì„±_ì‹œê°„": pd.Timestamp.now().isoformat()
            })
        
        # ë©”ì‹œì§€ ë¶„ì„ ê²°ê³¼
        if context.message_analysis:
            message_data = context.message_analysis
            df_data.append({
                "ë¦¬í¬íŠ¸_ìœ í˜•": "ë©”ì‹œì§€_ë¶„ì„",
                "ìµœê³ _ë¬¸êµ¬": str(message_data.get("best_message", "N/A"))[:50] + "...",
                "ìµœê³ _ì „í™˜ìœ¨": message_data.get("best_conversion_rate", 0),
                "ì´_ë¬¸êµ¬ìˆ˜": message_data.get("total_messages", 0),
                "ìƒì„±_ì‹œê°„": pd.Timestamp.now().isoformat()
            })
        
        # DataFrame ìƒì„± ë° ì €ì¥ (outputs í´ë”ì— ì €ì¥)
        datetime_prefix = get_datetime_prefix()
        if df_data:
            df = pd.DataFrame(df_data)
            reports_dir = get_reports_dir()
            
            csv_filename = f"{reports_dir}/{datetime_prefix}_data_analysis_report.csv"
            df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
        
        # JSON ë¦¬í¬íŠ¸ ìƒì„±
        json_report = {
            "report_metadata": {
                "generated_at": pd.Timestamp.now().isoformat(),
                "report_type": "ì¢…í•© ë°ì´í„° ë¶„ì„ ë¦¬í¬íŠ¸",
                "data_source": csv_file_path
            },
            "data_summary": context.data_info,
            "analysis_results": {
                "funnel_analysis": context.funnel_analysis,
                "message_analysis": context.message_analysis,
                "weekly_trends": context.weekly_trends
            },
            "insights": context.insights,
            "recommendations": context.recommendations,
            "final_report": context.final_report
        }
        
        # JSON íŒŒì¼ë¡œ ì €ì¥ (outputs/reports/{ë‚ ì§œ} í´ë”ì— ì €ì¥)
        from datetime import datetime
        today = datetime.now().strftime('%Y%m%d')
        reports_dir = f"{OUTPUT_DIR}/reports/{today}"
        os.makedirs(reports_dir, exist_ok=True)
        
        json_filename = f"{reports_dir}/{datetime_prefix}_comprehensive_data_report.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(json_report, f, ensure_ascii=False, indent=2)
        
        return f"ì¢…í•© ë°ì´í„° ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {csv_filename}, {json_filename}"
        
    except Exception as e:
        return f"ë°ì´í„° ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜: {str(e)}"

def prepare_funnel_message_analysis_data(csv_file_path: str, top_n: int = 5) -> str:
    """í¼ë„ë³„ ìƒìœ„/í•˜ìœ„ ë©”ì‹œì§€ ë°ì´í„°ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤ (LLM Analysis Agentìš©)
    
    Args:
        csv_file_path: CSV íŒŒì¼ ê²½ë¡œ
        top_n: ê° í¼ë„ì—ì„œ ì¶”ì¶œí•  ìƒìœ„/í•˜ìœ„ ë©”ì‹œì§€ ê°œìˆ˜
        
    Returns:
        JSON í˜•íƒœì˜ êµ¬ì¡°í™”ëœ ë°ì´í„°
    """
    try:
        import pandas as pd
        import json
        
        df = pd.read_csv(csv_file_path)
        
        print(f"ğŸ” í¼ë„ë³„ ë©”ì‹œì§€ ë°ì´í„° ì¤€ë¹„ ì¤‘ (ìƒìœ„/í•˜ìœ„ ê° {top_n}ê°œ)...")
        
        # ì „ì²´ ë°ì´í„° ì¤€ë¹„
        all_funnel_data = []
        funnel_stats = {}
        
        # í¼ë„ë³„ ë°ì´í„° ìˆ˜ì§‘
        for funnel in df['í¼ë„'].unique():
            if pd.isna(funnel):
                continue
                
            print(f"ğŸ“Š {funnel} í¼ë„ ë°ì´í„° ìˆ˜ì§‘...")
            
            # í•´ë‹¹ í¼ë„ì˜ ë°ì´í„°ë§Œ í•„í„°ë§
            funnel_data = df[df['í¼ë„'] == funnel]
            
            # Lift ê³„ì‚°
            funnel_data['exp_rate'] = funnel_data['ì‹¤í—˜êµ°_1ì¼ì´ë‚´_ì˜ˆì•½ìƒì„±'] / funnel_data['ì‹¤í—˜êµ°_ë°œì†¡']
            funnel_data['ctrl_rate'] = funnel_data['ëŒ€ì¡°êµ°_1ì¼ì´ë‚´_ì˜ˆì•½ìƒì„±'] / funnel_data['ëŒ€ì¡°êµ°_ë°œì†¡']
            funnel_data['lift'] = funnel_data['exp_rate'] - funnel_data['ctrl_rate']
            
            funnel_data_sorted = funnel_data.sort_values('ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨', ascending=False)
            
            if len(funnel_data_sorted) < 2:
                continue
            
            # í¼ë„ë³„ í†µê³„
            funnel_avg_exp = funnel_data['exp_rate'].mean()
            funnel_avg_ctrl = funnel_data['ctrl_rate'].mean()
            funnel_avg_lift = funnel_data['lift'].mean()
            
            funnel_stats[funnel] = {
                'avg_exp_conversion': round(funnel_avg_exp * 100, 2),
                'avg_ctrl_conversion': round(funnel_avg_ctrl * 100, 2),
                'avg_lift': round(funnel_avg_lift * 100, 2),
                'total_messages': len(funnel_data),
                'max_conversion': round(funnel_data['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].max(), 2),
                'min_conversion': round(funnel_data['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].min(), 2)
            }
            
            # ìƒìœ„ Nê°œ ë¬¸êµ¬ ì„ íƒ
            top_messages = funnel_data_sorted.head(top_n)
            
            # í•˜ìœ„ Nê°œ ë¬¸êµ¬ ì„ íƒ
            bottom_messages = funnel_data_sorted.tail(top_n)
            
            # ìƒìœ„ ë¬¸êµ¬ ë°ì´í„°
            for i, (idx, row) in enumerate(top_messages.iterrows()):
                all_funnel_data.append({
                    "funnel": funnel,
                    "message": str(row['ë¬¸êµ¬']),
                    "conversion_rate": round(row['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'], 2),
                    "exp_rate": round(row['exp_rate'] * 100, 2),
                    "ctrl_rate": round(row['ctrl_rate'] * 100, 2),
                    "lift": round(row['lift'] * 100, 2),
                    "channel": str(row['ì±„ë„']) if 'ì±„ë„' in row else "N/A",
                    "length": len(str(row['ë¬¸êµ¬'])),
                    "rank": i + 1,
                    "group": "high_performing",
                    "funnel_avg_exp": round(funnel_avg_exp * 100, 2),
                    "funnel_avg_lift": round(funnel_avg_lift * 100, 2)
                })
            
            # í•˜ìœ„ ë¬¸êµ¬ ë°ì´í„°
            for i, (idx, row) in enumerate(bottom_messages.iterrows()):
                all_funnel_data.append({
                    "funnel": funnel,
                    "message": str(row['ë¬¸êµ¬']),
                    "conversion_rate": round(row['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'], 2),
                    "exp_rate": round(row['exp_rate'] * 100, 2),
                    "ctrl_rate": round(row['ctrl_rate'] * 100, 2),
                    "lift": round(row['lift'] * 100, 2),
                    "channel": str(row['ì±„ë„']) if 'ì±„ë„' in row else "N/A",
                    "length": len(str(row['ë¬¸êµ¬'])),
                    "rank": i + 1,
                    "group": "low_performing",
                    "funnel_avg_exp": round(funnel_avg_exp * 100, 2),
                    "funnel_avg_lift": round(funnel_avg_lift * 100, 2)
                })
        
        # ê²°ê³¼ êµ¬ì¡°í™”
        result = {
            "analysis_metadata": {
                "total_funnels": len(funnel_stats),
                "total_messages": len(all_funnel_data),
                "top_n_per_funnel": top_n,
                "analysis_type": "funnel_message_comparison"
            },
            "funnel_statistics": funnel_stats,
            "messages": all_funnel_data
        }
        
        print(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(funnel_stats)}ê°œ í¼ë„, {len(all_funnel_data)}ê°œ ë©”ì‹œì§€")
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_result = {
            "error": f"ë°ì´í„° ì¤€ë¹„ ì˜¤ë¥˜: {str(e)}",
            "analysis_metadata": {},
            "funnel_statistics": {},
            "messages": []
        }
        return json.dumps(error_result, ensure_ascii=False)

def prepare_funnel_quantile_data(csv_file_path: str) -> str:
    """í¼ë„ë³„ ë¶„ìœ„ìˆ˜ ê³„ì‚° ë° ë°ì´í„° ì¤€ë¹„"""
    try:
        import pandas as pd
        import numpy as np
        
        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(csv_file_path)
        
        # Lift ê³„ì‚°
        df['exp_rate'] = df['ì‹¤í—˜êµ°_1ì¼ì´ë‚´_ì˜ˆì•½ìƒì„±'] / df['ì‹¤í—˜êµ°_ë°œì†¡']
        df['ctrl_rate'] = df['ëŒ€ì¡°êµ°_1ì¼ì´ë‚´_ì˜ˆì•½ìƒì„±'] / df['ëŒ€ì¡°êµ°_ë°œì†¡']
        df['lift'] = df['exp_rate'] - df['ctrl_rate']
        
        # í¼ë„ë³„ í†µê³„ ê³„ì‚° (ì „ì²´ ì „í™˜ìœ¨ ê¸°ì¤€)
        funnel_stats = df.groupby('í¼ë„').agg({
            'ì‹¤í—˜êµ°_1ì¼ì´ë‚´_ì˜ˆì•½ìƒì„±': 'sum',
            'ì‹¤í—˜êµ°_ë°œì†¡': 'sum',
            'ëŒ€ì¡°êµ°_1ì¼ì´ë‚´_ì˜ˆì•½ìƒì„±': 'sum',
            'ëŒ€ì¡°êµ°_ë°œì†¡': 'sum'
        }).reset_index()
        
        # í¼ë„ë³„ ì „ì²´ ì „í™˜ìœ¨ ë° Lift ê³„ì‚°
        funnel_stats['exp_rate'] = funnel_stats['ì‹¤í—˜êµ°_1ì¼ì´ë‚´_ì˜ˆì•½ìƒì„±'] / funnel_stats['ì‹¤í—˜êµ°_ë°œì†¡']
        funnel_stats['ctrl_rate'] = funnel_stats['ëŒ€ì¡°êµ°_1ì¼ì´ë‚´_ì˜ˆì•½ìƒì„±'] / funnel_stats['ëŒ€ì¡°êµ°_ë°œì†¡']
        funnel_stats['lift'] = funnel_stats['exp_rate'] - funnel_stats['ctrl_rate']
        funnel_stats['campaign_count'] = df.groupby('í¼ë„').size().reset_index(name='count')['count']
        
        # 3ë¶„ìœ„ìˆ˜ ê¸°ì¤€ ê³„ì‚° (í¼ë„ë³„ ì „ì²´ Lift ê¸°ì¤€)
        q33 = funnel_stats['lift'].quantile(0.33)
        q67 = funnel_stats['lift'].quantile(0.67)
        
        # ê·¸ë£¹ë³„ ë¶„ë¥˜
        high_group = funnel_stats[funnel_stats['lift'] >= q67].copy()
        medium_group = funnel_stats[(funnel_stats['lift'] >= q33) & (funnel_stats['lift'] < q67)].copy()
        low_group = funnel_stats[funnel_stats['lift'] < q33].copy()
        
        # ê° ê·¸ë£¹ë³„ ìƒì„¸ ë°ì´í„° ì¤€ë¹„
        def prepare_group_data(group_df, group_name):
            group_data = {
                "group_name": group_name,
                "funnels": [],
                "all_messages": []
            }
            
            for _, row in group_df.iterrows():
                funnel = row['í¼ë„']
                funnel_data = df[df['í¼ë„'] == funnel]
                
                # í•´ë‹¹ í¼ë„ì˜ ìƒìœ„ ì„±ê³¼ ë¬¸êµ¬ (Lift ê¸°ì¤€)
                top_messages = funnel_data.nlargest(5, 'lift')[['ë¬¸êµ¬', 'lift', 'exp_rate', 'ctrl_rate']]
                
                funnel_info = {
                    "funnel": funnel,
                    "lift": round(row['lift'] * 100, 2),
                    "exp_rate": round(row['exp_rate'] * 100, 2),
                    "ctrl_rate": round(row['ctrl_rate'] * 100, 2),
                    "campaign_count": int(row['campaign_count']),
                    "top_messages": []
                }
                
                for _, msg_row in top_messages.iterrows():
                    message_text = str(msg_row['ë¬¸êµ¬'])
                    message_data = {
                        "message": message_text,
                        "lift": round(msg_row['lift'] * 100, 2),  # ê°œë³„ ë¬¸êµ¬ì˜ Lift ì‚¬ìš©
                        "exp_rate": round(msg_row['exp_rate'] * 100, 2),  # ê°œë³„ ë¬¸êµ¬ì˜ ì‹¤í—˜êµ° ì „í™˜ìœ¨ ì‚¬ìš©
                        "ctrl_rate": round(msg_row['ctrl_rate'] * 100, 2)  # ê°œë³„ ë¬¸êµ¬ì˜ ëŒ€ì¡°êµ° ì „í™˜ìœ¨ ì‚¬ìš©
                    }
                    funnel_info["top_messages"].append(message_data)
                    group_data["all_messages"].append(message_data)
                
                group_data["funnels"].append(funnel_info)
            
            return group_data
        
        # ê·¸ë£¹ë³„ ë°ì´í„° ì¤€ë¹„
        high_data = prepare_group_data(high_group, "high")
        medium_data = prepare_group_data(medium_group, "medium")
        low_data = prepare_group_data(low_group, "low")
        
        # ì¢…í•© ë°ì´í„°
        quantile_data = {
            "quantile_thresholds": {
                "q33": round(q33 * 100, 2),
                "q67": round(q67 * 100, 2)
            },
            "high_performance_group": high_data,
            "medium_performance_group": medium_data,
            "low_performance_group": low_data
        }
        
        # ê²°ê³¼ ì €ì¥
        from datetime import datetime
        today = datetime.now().strftime('%Y%m%d')
        reports_dir = f"outputs/reports/{today}"
        os.makedirs(reports_dir, exist_ok=True)
        
        quantile_path = f"{reports_dir}/{datetime.now().strftime('%y%m%d_%H%M')}_funnel_quantile_data.json"
        with open(quantile_path, 'w', encoding='utf-8') as f:
            json.dump(quantile_data, f, ensure_ascii=False, indent=2)
        
        return json.dumps(quantile_data, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return f"í¼ë„ë³„ ë¶„ìœ„ìˆ˜ ë°ì´í„° ì¤€ë¹„ ì˜¤ë¥˜: {str(e)}"
