"""Comprehensive Agent - ëª¨ë“  ë¶„ì„ì„ í†µí•©í•˜ëŠ” ì¢…í•© ë¶„ì„ Agent"""

import pandas as pd
import json
import asyncio
from typing import Dict, Any, List
from google.adk.agents import Agent
from google.adk.models.lite_llm import LiteLlm
from google.adk.runners import Runner
from google.adk.sessions import InMemorySessionService
from google.genai import types

from settings import get_logger, settings
from analysis_context import analysis_context
from data_analysis_functions import (
    analyze_conversion_performance,
    analyze_message_effectiveness,
    analyze_funnel_performance,
    analyze_funnel_message_effectiveness,
    analyze_message_patterns_by_funnel,
    analyze_single_message_llm,
    analyze_messages_by_funnel_llm,
    analyze_message_effectiveness_reasons
)
from column_descriptions import SIMPLE_COLUMN_DESCRIPTIONS

logger = get_logger(__name__)

# Azure OpenAI ì„¤ì •
azure_llm = LiteLlm(
    model=f"azure/{settings.AZURE_OPENAI_DEPLOYMENT_NAME}",
    api_key=settings.AZURE_OPENAI_API_KEY,
    api_base=settings.AZURE_OPENAI_ENDPOINT,
    api_version=settings.AZURE_OPENAI_API_VERSION,
)

# =============================================================================
# í†µí•© ë¶„ì„ ë„êµ¬ë“¤
# =============================================================================

def comprehensive_data_analysis(csv_file_path: str) -> str:
    """ì¢…í•© ë°ì´í„° ë¶„ì„ ìˆ˜í–‰"""
    try:
        print("ğŸ” ì¢…í•© ë°ì´í„° ë¶„ì„ ì‹œì‘...")
        
        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(csv_file_path)
        
        # 1. ë°ì´í„° ì´í•´ë„ ë¶„ì„
        data_understanding = {
            "data_shape": df.shape,
            "columns": df.columns.tolist(),
            "missing_values": df.isnull().sum().to_dict(),
            "data_types": df.dtypes.to_dict(),
            "basic_stats": df.describe().to_dict()
        }
        analysis_context.update_data_understanding(data_understanding)
        
        # 2. í†µê³„ ë¶„ì„
        statistical_results = {
            "conversion_analysis": analyze_conversion_performance(df),
            "message_analysis": analyze_message_effectiveness(df),
            "funnel_analysis": analyze_funnel_performance(df),
            "funnel_message_analysis": analyze_funnel_message_effectiveness(df),
            "pattern_analysis": analyze_message_patterns_by_funnel(df)
        }
        analysis_context.update_statistical_analysis(statistical_results)
        
        # 3. LLM ë¶„ì„
        llm_results = {
            "message_llm_analysis": analyze_messages_by_funnel_llm(df, sample_size=3),
            "effectiveness_reasons": analyze_message_effectiveness_reasons(df)
        }
        analysis_context.update_llm_analysis(llm_results)
        
        # 4. í†µí•© ì¸ì‚¬ì´íŠ¸ ìƒì„±
        integrated_insights = analysis_context.integrate_insights()
        
        return f"ì¢…í•© ë¶„ì„ ì™„ë£Œ: {len(integrated_insights)}ê°œ í†µí•© ì¸ì‚¬ì´íŠ¸ ìƒì„±"
        
    except Exception as e:
        return f"ì¢…í•© ë¶„ì„ ì˜¤ë¥˜: {str(e)}"

def generate_insights_report(csv_file_path: str) -> str:
    """ì¸ì‚¬ì´íŠ¸ ë³´ê³ ì„œ ìƒì„±"""
    try:
        print("ğŸ“Š ì¸ì‚¬ì´íŠ¸ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        # í†µí•© ì¸ì‚¬ì´íŠ¸ ìƒì„±
        integrated_insights = analysis_context.integrate_insights()
        
        # ìµœì¢… ë³´ê³ ì„œ ìƒì„±
        final_report = analysis_context.generate_final_report()
        
        # JSON ì €ì¥
        analysis_context.save_to_json("comprehensive_analysis_report.json")
        
        # DataFrame ì €ì¥
        analysis_context.save_to_dataframe("comprehensive_analysis_results.csv")
        
        return f"ì¸ì‚¬ì´íŠ¸ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {len(final_report)}ê°œ ì„¹ì…˜"
        
    except Exception as e:
        return f"ì¸ì‚¬ì´íŠ¸ ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {str(e)}"

def analyze_specific_funnel(csv_file_path: str, funnel_name: str) -> str:
    """íŠ¹ì • í¼ë„ ìƒì„¸ ë¶„ì„"""
    try:
        df = pd.read_csv(csv_file_path)
        funnel_data = df[df['í¼ë„'] == funnel_name]
        
        if len(funnel_data) == 0:
            return f"í¼ë„ '{funnel_name}' ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # í¼ë„ë³„ ìƒì„¸ ë¶„ì„
        funnel_analysis = {
            "funnel_name": funnel_name,
            "total_campaigns": len(funnel_data),
            "avg_conversion_rate": funnel_data['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].mean(),
            "best_message": funnel_data.loc[funnel_data['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].idxmax(), 'ë¬¸êµ¬'],
            "best_conversion_rate": funnel_data['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].max(),
            "channel_distribution": funnel_data['ì±„ë„'].value_counts().to_dict()
        }
        
        # Contextì— ì¶”ê°€
        analysis_context.update_statistical_analysis({f"funnel_{funnel_name}": funnel_analysis})
        
        return f"í¼ë„ '{funnel_name}' ë¶„ì„ ì™„ë£Œ: {funnel_analysis['total_campaigns']}ê°œ ìº í˜ì¸"
        
    except Exception as e:
        return f"í¼ë„ ë¶„ì„ ì˜¤ë¥˜: {str(e)}"

def compare_experiment_vs_control(csv_file_path: str) -> str:
    """ì‹¤í—˜êµ° vs ëŒ€ì¡°êµ° ìƒì„¸ ë¹„êµ"""
    try:
        df = pd.read_csv(csv_file_path)
        
        # ì‹¤í—˜êµ° vs ëŒ€ì¡°êµ° ë¹„êµ
        comparison = {
            "experiment_avg_conversion": df['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].mean(),
            "control_avg_conversion": df['ëŒ€ì¡°êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].mean(),
            "lift_percentage": df['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].mean() - df['ëŒ€ì¡°êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].mean(),
            "statistical_significance": "ë¶„ì„ í•„ìš”",
            "best_performing_campaigns": df.nlargest(5, 'ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨')[['í¼ë„', 'ë¬¸êµ¬', 'ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨']].to_dict('records')
        }
        
        # Contextì— ì¶”ê°€
        analysis_context.update_statistical_analysis({"experiment_control_comparison": comparison})
        
        return f"ì‹¤í—˜êµ° vs ëŒ€ì¡°êµ° ë¹„êµ ì™„ë£Œ: Lift {comparison['lift_percentage']:.2f}%"
        
    except Exception as e:
        return f"ì‹¤í—˜êµ° vs ëŒ€ì¡°êµ° ë¹„êµ ì˜¤ë¥˜: {str(e)}"

def generate_actionable_recommendations(csv_file_path: str) -> str:
    """ì‹¤í–‰ ê°€ëŠ¥í•œ ì¶”ì²œì‚¬í•­ ìƒì„±"""
    try:
        print("ğŸ’¡ ì‹¤í–‰ ê°€ëŠ¥í•œ ì¶”ì²œì‚¬í•­ ìƒì„± ì¤‘...")
        
        df = pd.read_csv(csv_file_path)
        
        # ë°ì´í„° ê¸°ë°˜ ì¶”ì²œì‚¬í•­ ìƒì„±
        recommendations = []
        
        # 1. í¼ë„ë³„ ì¶”ì²œ
        funnel_performance = df.groupby('í¼ë„')['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].agg(['mean', 'count']).round(3)
        best_funnel = funnel_performance['mean'].idxmax()
        worst_funnel = funnel_performance['mean'].idxmin()
        
        recommendations.append({
            "category": "í¼ë„ ìµœì í™”",
            "priority": "high",
            "recommendation": f"'{best_funnel}' í¼ë„ì˜ ì„±ê³µ íŒ¨í„´ì„ '{worst_funnel}' í¼ë„ì— ì ìš©",
            "expected_impact": f"ì „í™˜ìœ¨ {funnel_performance.loc[worst_funnel, 'mean']:.1f}% â†’ {funnel_performance.loc[best_funnel, 'mean']:.1f}% ê°œì„  ê°€ëŠ¥"
        })
        
        # 2. ë¬¸êµ¬ë³„ ì¶”ì²œ
        message_performance = df.groupby('ë¬¸êµ¬')['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].agg(['mean', 'count']).round(3)
        best_message = message_performance['mean'].idxmax()
        
        recommendations.append({
            "category": "ë©”ì‹œì§€ ìµœì í™”",
            "priority": "high",
            "recommendation": f"ê³ ì„±ê³¼ ë¬¸êµ¬ íŒ¨í„´ ë¶„ì„ ë° ì ìš©",
            "best_message_sample": best_message[:100] + "...",
            "conversion_rate": f"{message_performance.loc[best_message, 'mean']:.1f}%"
        })
        
        # 3. ì±„ë„ë³„ ì¶”ì²œ
        channel_performance = df.groupby('ì±„ë„')['ì‹¤í—˜êµ°_ì˜ˆì•½ì „í™˜ìœ¨'].agg(['mean', 'count']).round(3)
        best_channel = channel_performance['mean'].idxmax()
        
        recommendations.append({
            "category": "ì±„ë„ ìµœì í™”",
            "priority": "medium",
            "recommendation": f"'{best_channel}' ì±„ë„ì˜ ì„±ê³µ ìš”ì†Œë¥¼ ë‹¤ë¥¸ ì±„ë„ì— ì ìš©",
            "channel_performance": channel_performance.to_dict()
        })
        
        # Contextì— ì¶”ê°€
        analysis_context.update_statistical_analysis({"actionable_recommendations": recommendations})
        
        return f"ì‹¤í–‰ ê°€ëŠ¥í•œ ì¶”ì²œì‚¬í•­ {len(recommendations)}ê°œ ìƒì„± ì™„ë£Œ"
        
    except Exception as e:
        return f"ì¶”ì²œì‚¬í•­ ìƒì„± ì˜¤ë¥˜: {str(e)}"

# =============================================================================
# Comprehensive Agent ìƒì„±
# =============================================================================

comprehensive_agent = Agent(
    name="comprehensive_agent",
    model=azure_llm,
    description="CRM ë°ì´í„°ì˜ ì¢…í•© ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  í†µí•© ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
    instruction=f"""
    # ì¢…í•© CRM ë¶„ì„ ì „ë¬¸ê°€
    
    ## ì—­í• 
    ë‹¹ì‹ ì€ ì˜ì¹´ì˜ Senior Data Analystë¡œì„œ, CRM ìº í˜ì¸ ë°ì´í„°ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ 
    ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ì™€ ì‹¤í–‰ ê°€ëŠ¥í•œ ì¶”ì²œì‚¬í•­ì„ ì œê³µí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    
    ## ì»¬ëŸ¼ ì„¤ëª…
    {SIMPLE_COLUMN_DESCRIPTIONS}
    
    ## ë¶„ì„ ëª©í‘œ
    1. ë°ì´í„° ì´í•´ë„ ë° í’ˆì§ˆ ë¶„ì„
    2. í†µê³„ì  ì„±ê³¼ ë¶„ì„ (ì „í™˜ìœ¨, í¼ë„ë³„, ì±„ë„ë³„)
    3. LLM ê¸°ë°˜ ë¬¸êµ¬ íš¨ê³¼ì„± ë¶„ì„
    4. ì‹¤í—˜êµ° vs ëŒ€ì¡°êµ° ë¹„êµ ë¶„ì„
    5. í†µí•© ì¸ì‚¬ì´íŠ¸ ë° ì¶”ì²œì‚¬í•­ ë„ì¶œ
    
    ## ì¶œë ¥ í˜•ì‹
    - ì¢…í•© ë¶„ì„ ê²°ê³¼ ìš”ì•½
    - í†µê³„ì  ì§€í‘œ ë° ì„±ê³¼ ë¶„ì„
    - ë¬¸êµ¬ íš¨ê³¼ì„± ë° íŒ¨í„´ ë¶„ì„
    - ì‹¤í–‰ ê°€ëŠ¥í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ì¶”ì²œì‚¬í•­
    - DataFrame/JSON í˜•íƒœì˜ êµ¬ì¡°í™”ëœ ê²°ê³¼
    
    ## Context ê³µìœ 
    - ëª¨ë“  ë¶„ì„ ê²°ê³¼ëŠ” Contextì— ì €ì¥ë˜ì–´ í›„ì† ë¶„ì„ì—ì„œ í™œìš©
    - ì´ì „ ë¶„ì„ ê²°ê³¼ë¥¼ ì°¸ì¡°í•˜ì—¬ ë” ì •í™•í•œ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
    - í†µí•©ëœ ê´€ì ì—ì„œ ì¢…í•©ì ì¸ ë¶„ì„ ìˆ˜í–‰
    """,
    tools=[
        comprehensive_data_analysis,
        generate_insights_report,
        analyze_specific_funnel,
        compare_experiment_vs_control,
        generate_actionable_recommendations
    ],
)

# =============================================================================
# ì‹¤í–‰ í•¨ìˆ˜
# =============================================================================

async def run_agent_with_llm(agent, query: str, agent_name: str):
    """LLM Agent ì‹¤í–‰"""
    user_id = "comprehensive_user"
    session_id = f"session_{agent_name}"

    # ì„¸ì…˜ ì„œë¹„ìŠ¤ ìƒì„±
    session_service = InMemorySessionService()

    # Runner ìƒì„±
    runner = Runner(
        agent=agent,
        session_service=session_service,
        app_name=f"{agent_name}_app"
    )

    # ì„¸ì…˜ ìƒì„±
    await session_service.create_session(
        app_name=f"{agent_name}_app",
        user_id=user_id,
        session_id=session_id
    )

    print(f"ğŸ¤– {agent_name} Agent ì‹¤í–‰ ì¤‘...")

    content = types.Content(role="user", parts=[types.Part(text=query)])

    async for event in runner.run_async(user_id=user_id, session_id=session_id, new_message=content):
        if event.is_final_response():
            if event.content and event.content.parts:
                response = event.content.parts[0].text
                print(f"ğŸ“ {agent_name} ì‘ë‹µ: {response}")
            break

async def run_comprehensive_analysis():
    """ì¢…í•© ë¶„ì„ ì‹œìŠ¤í…œ ì‹¤í–‰"""
    print("ğŸš€ ì¢…í•© ë¶„ì„ ì‹œìŠ¤í…œ ì‹œì‘")
    print("=" * 80)

    # CSV íŒŒì¼ ê²½ë¡œ
    csv_file = "clean_df_renamed.csv"

    # ì¢…í•© ë¶„ì„ ì¿¼ë¦¬
    comprehensive_query = f"""
    ë‹¤ìŒ CRM ë°ì´í„°ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”: {csv_file}

    ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¼ ì¢…í•© ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”:
    
    1. **ë°ì´í„° ì´í•´ë„ ë¶„ì„**
       - ë°ì´í„° êµ¬ì¡°, í’ˆì§ˆ, ê¸°ë³¸ í†µê³„ ë¶„ì„
       - ì»¬ëŸ¼ë³„ íŠ¹ì„± ë° ê²°ì¸¡ì¹˜ ë¶„ì„
    
    2. **í†µê³„ì  ì„±ê³¼ ë¶„ì„**
       - ì‹¤í—˜êµ° vs ëŒ€ì¡°êµ° ì „í™˜ìœ¨ ë¹„êµ
       - í¼ë„ë³„ ì„±ê³¼ ë¶„ì„
       - ì±„ë„ë³„ ì„±ê³¼ ë¶„ì„
       - ë¬¸êµ¬ë³„ íš¨ê³¼ì„± ë¶„ì„
    
    3. **LLM ê¸°ë°˜ ë¬¸êµ¬ ë¶„ì„**
       - ë¬¸êµ¬ êµ¬ì¡° ë° íŒ¨í„´ ë¶„ì„
       - í†¤ì•¤ë§¤ë„ˆ ë° í‚¤ì›Œë“œ ë¶„ì„
       - íš¨ê³¼ì„± ì´ìœ  ë¶„ì„
    
    4. **í†µí•© ì¸ì‚¬ì´íŠ¸ ë„ì¶œ**
       - ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•œ ì¸ì‚¬ì´íŠ¸
       - êµì°¨ ë¶„ì„ì„ í†µí•œ ìƒˆë¡œìš´ ë°œê²¬
       - ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ í‰ê°€
    
    5. **ì‹¤í–‰ ê°€ëŠ¥í•œ ì¶”ì²œì‚¬í•­**
       - í¼ë„ ìµœì í™” ë°©ì•ˆ
       - ë©”ì‹œì§€ ê°œì„  ì œì•ˆ
       - ì±„ë„ ì „ëµ ìˆ˜ì •
       - ìš°ì„ ìˆœìœ„ë³„ ì•¡ì…˜ ì•„ì´í…œ
    
    6. **ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±**
       - JSON í˜•íƒœì˜ êµ¬ì¡°í™”ëœ ê²°ê³¼
       - DataFrame í˜•íƒœì˜ ì •ëŸ‰ì  ê²°ê³¼
       - ê²½ì˜ì§„ ìš”ì•½ ë³´ê³ ì„œ
    
    ê° ë‹¨ê³„ë§ˆë‹¤ ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ ì‹¤ì œ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³ , 
    ëª¨ë“  ê²°ê³¼ë¥¼ Contextì— ì €ì¥í•˜ì—¬ í†µí•©ì ì¸ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”.
    """

    await run_agent_with_llm(comprehensive_agent, comprehensive_query, "comprehensive_analysis")

    print("\nâœ… ì¢…í•© ë¶„ì„ ì‹œìŠ¤í…œ ì™„ë£Œ!")
    print("=" * 80)

if __name__ == "__main__":
    asyncio.run(run_comprehensive_analysis())
